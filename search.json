[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jian Wei CHEONG",
    "section": "",
    "text": "ORCID\n  \n  \n    \n     Research group\n  \n\n      \nHi, I am Jian Wei, a Physics research fellow in Nanyang Technological University, Singapore.\nMy research interests are in the fields of quantum information and quantum thermodynamics.\nI play with Linux and do programming for fun.\nI believe in the efficiency of workflow. Everything should be as painless as possible, doing more with less.\n\n    \n    \n  \n\n\n\n\nRecent posts\n\n\n\n\nDate\nTitle\nReading Time\n\n\n\n\nMay 1, 2024\nLife as an entropic conspiracy\n8 min\n\n\nSep 5, 2023\nHaskell for numerical computation?\n15 min\n\n\nApr 1, 2023\nWhy I recommend Fedora for new Linux users\n5 min\n\n\n\n\nNo matching items\n\n\nsee more\n\n\n\nList of publications\n\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Non-Markovian refrigeration and heat flow in the quantum switch, Physical Review A, 110(2), 022220 (2024).\nL. Y. Chew, A. Pradana, L. He, and J. W. Cheong, Stochastic thermodynamics of finite-tape information ratchet, European Physical Journal Special Topics (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Effects of non-Markovianity on daemonic ergotropy in the quantum switch, Physical Review A, 108(1), 012201 (2023).\nL. He, J. W. Cheong, A. Pradana, and L. Y. Chew, Effects of correlation in an information ratchet with finite tape, Physical Review E, 107(2), 024130 (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Communication advantage of quantum compositions of channels from non-Markovianity, Physical Review A, 106(5), 052410 (2022).\nL. He, A. Pradana, J. W. Cheong, and L. Y. Chew, Information processing second law for an information ratchet with finite tape, Physical Review E, 105(5), 054131 (2022).\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Doctor of Philosophy, Physics  Nanyang Technological University (NTU), Singapore\nBachelor of Science, Physics (Honours)  Nanyang Technological University (NTU), Singapore\nDiploma, Electrical Engineering  Ngee Ann Polytechnic, Singapore\n\n2019 - 2023\n 2015 - 2019\n 2010 - 2013"
  },
  {
    "objectID": "cv.html#professional-qualifications",
    "href": "cv.html#professional-qualifications",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Doctor of Philosophy, Physics  Nanyang Technological University (NTU), Singapore\nBachelor of Science, Physics (Honours)  Nanyang Technological University (NTU), Singapore\nDiploma, Electrical Engineering  Ngee Ann Polytechnic, Singapore\n\n2019 - 2023\n 2015 - 2019\n 2010 - 2013"
  },
  {
    "objectID": "cv.html#professional-appointments",
    "href": "cv.html#professional-appointments",
    "title": "Curriculum Vitae",
    "section": "Professional Appointments",
    "text": "Professional Appointments\n\n\nResearch Fellow  Nanyang Technological University (NTU), Singapore\nProject Officer  Nanyang Technological University (NTU), Singapore\nTeaching Assistant  PH3101 Quantum Mechanics 2  PH1199 Physics Lab 1B  PH1198 Physics Lab 1A  Nanyang Technological University (NTU), Singapore\nIntern  ST Electronics, Singapore\n\n2024 - current\n 2023 - 2024\n  2021  2020  2019\n 2012"
  },
  {
    "objectID": "cv.html#awards-and-achievements",
    "href": "cv.html#awards-and-achievements",
    "title": "Curriculum Vitae",
    "section": "Awards and Achievements",
    "text": "Awards and Achievements\n\n\nShort-speech Contest Best Presentation  PAP701 Graduate seminar module  Nanyang Technological University (NTU), Singapore\nDean’s List (top 5% of cohort)  Nanyang Technological University (NTU), Singapore\nNTU President Research Scholar (completing URECA)  Nanyang Technological University (NTU), Singapore\nDirector’s List (top 5% of cohort)  Ngee Ann Polytechnic, Singapore\nBest Performance (top student of cohort)  Programmable Logic Device  Digital Electronics & Practice  Ngee Ann Polytechnic, Singapore\n\n2019\n  2017/2018\n 2016/2017\n 2011\n  2011  2010"
  },
  {
    "objectID": "cv.html#list-of-publications",
    "href": "cv.html#list-of-publications",
    "title": "Curriculum Vitae",
    "section": "List of Publications",
    "text": "List of Publications\n\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Non-Markovian refrigeration and heat flow in the quantum switch, Physical Review A, 110(2), 022220 (2024).\nL. Y. Chew, A. Pradana, L. He, and J. W. Cheong, Stochastic thermodynamics of finite-tape information ratchet, European Physical Journal Special Topics (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Effects of non-Markovianity on daemonic ergotropy in the quantum switch, Physical Review A, 108(1), 012201 (2023).\nL. He, J. W. Cheong, A. Pradana, and L. Y. Chew, Effects of correlation in an information ratchet with finite tape, Physical Review E, 107(2), 024130 (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Communication advantage of quantum compositions of channels from non-Markovianity, Physical Review A, 106(5), 052410 (2022).\nL. He, A. Pradana, J. W. Cheong, and L. Y. Chew, Information processing second law for an information ratchet with finite tape, Physical Review E, 105(5), 054131 (2022)."
  },
  {
    "objectID": "cv.html#technical-experience",
    "href": "cv.html#technical-experience",
    "title": "Curriculum Vitae",
    "section": "Technical Experience",
    "text": "Technical Experience\nComputer languages  Python, Julia, R, C, C++, MATLAB, Haskell, Racket, LaTeX, Typst, HTML, CSS, bash, POSIX sh\nMiscellaneous software  Arduino, Google Sketchup, Autodesk Fusion360, Autodesk AutoCAD, Autodesk EAGLE, Origin Pro, National Instruments LabVIEW\n\nMiscellaneous projects\nStrain estimation for hazard forecastings before and after 2011 Japan Tohoku earthquake  ES7008 Geophysical Data Analysis, NTU  Analyzed seismic GPS displacement data in Python.  Estimated seismic strains with velocity fields using Delaunay triangulation.  Demonstrated correlations between earthquake event hotspots and strain hotspots, before and after Tohoku earthquake.\nVariations in statistical complexity of genome sequences across species  CE7412 Computational and Systems Biology, NTU  Analyzed genome sequences of human, chimpanzee, rhesus macaque, dog, and fruit fly, from GenBank assembly in R.  Applied the Baum-Welch algorithm and Akaike information criterion to compute the average statistical complexity of their genomes.  Suggested that increased biological complexity corresponds to decreased statistical complexity in genomes.\nDetecting adversarial attack of deep neural networks for image recognition from image complexity  PH3502 Chaotic Dynamical Systems, NTU  Trained image recognition deep neural networks with MNIST, Fashion-MNIST, and CIFAR10 datasets in Python.  Applied adversarial attacks such as Fast Gradient Sign Method (FGSM), DeepFool, One Pixel Attack, Jacobian-Based Saliency Map Attack (JSMA).  Showed that FGSM and DeepFool can be detected from its increased image complexity.\nMonte Carlo photon transport in multi-layered biological tissues  PH4505 Computational Physics, NTU  Simulated photon transports in biological tissues by means of random walk in Python.  Demonstrated the applications of computational methods on medical areas such as biomedical imaging and photon therapy.\nMonte Carlo simulation of periodic-driven Brownian particles  PAP723 Numerical Methods for Physicists, NTU  Simulated 2D toy model of attractive Brownian particles that obeys the Arrhenius equation for the formation and destruction of bonds.  Demonstrated that the system tends to configurations that result in increased entropy production when driven with a periodic driving force.\nDesigning, programming, 3D printing, and building a hovering quadcopter drone  Making and Tinkering Lite 1, NTU  Programmed a Arduino microcontroller.  Designed printed circuit board (PCB) in Autodesk EAGLE.  Designed and 3D printed drone in Autodesk Fusion360.  Simulated physical system in COMSOL Multiphysics."
  },
  {
    "objectID": "posts/haskell-numerical.html",
    "href": "posts/haskell-numerical.html",
    "title": "Haskell for numerical computation?",
    "section": "",
    "text": "Recently, I have been experimenting with Haskell which I find to be quite enjoyable to program in, thanks to its unique programming paradigm. So much so that I am considering using it for numerical computations in my research.\nSide-effects refers to having an effect on the “outside world” such as printing a string to terminal or writing to a file etc. Of course, we do want some side-effects for the program to do anything useful. In Haskell, side-effects are handled by monads, which is a beast of a topic on its own. Monads allow the pure and impure (side-effects) to be clearly separated in Haskell.\nHaskell (named after logician Haskell Curry) is a purely functional programming language in the mathematical sense. Every function in Haskell simply takes an input and returns an output—nothing more, nothing less. It cannot mutate variables, and it cannot produce side-effects.\nPurely functional programming languages are typically declarative, i.e., one does not write step-by-step instructions like imperative languages such as Python, C, C++, etc. For example, there are no loops in Haskell; one has to rely on recursion or functions such as ‘fold’ and ‘scan’.\nThe two main reasons for my consideration to use it for numerical computations are:\n\nConciseness: Haskell’s declarative style greatly increase its conciseness. Prototyping and experimentation are frequent in computational research. Concise codes can reduce this programming time, and allow faster prototyping.\nSafety and correctness: A small bug in numerical computational would not raise any error. Instead, it manifests in computed results or plots, masquerading as valid or even novel results. If these bugs are not identified, one might publish a paper with faux results.\n\nHaskell’s static typing can serve as an additional check to reduce bugs.\nIts declarative approach means that codes are written in easily verifiable expressions, like in mathematics, rather than imperative statements that are often prone to bugs or human error.\nImmutability and the absence of side-effects in functions can also reduce hidden bugs. For instance, the risk of a function modifying a variable without the coder’s knowledge is mitigated, and parallel computations are less likely to produce incorrect results due to variable mutations in multiple threads.\n\n\nI am neither a software developer nor am I computer science trained, and so some technical aspects of the language might escape me. Therefore, in this article, we will only examine some simple examples to compare traditional imperative languages like Python with purely functional Haskell."
  },
  {
    "objectID": "posts/haskell-numerical.html#sum-recursion",
    "href": "posts/haskell-numerical.html#sum-recursion",
    "title": "Haskell for numerical computation?",
    "section": "Sum: recursion",
    "text": "Sum: recursion\nIgnoring the fact that the function sum is built-in in both Python and Haskell, let’s see how one might write a sum function that sum all the elements in a list.\n\nPythonHaskell\n\n\ndef sum(xs):\n    sum_all = 0\n    for x in xs:\n        sum_all += x\n    return sum_all\n\n\nsum :: [Float] -&gt; Float\nsum [] = 0\nsum (x:xs) = x + sum xs\n\n\n\nIn the imperative Python example, we tell the program what to do step-by-step: first initialize an accumulator sum_all to 0, then loop through each elements in the list and add them to the accumulator.\nOne might notice that functions are applied without parenthesis, i.e., applying a function foo on a variable x, is typically foo(x), but would be foo x in Haskell. The reason for this has something to do with the concept of “currying” which we will see later.\nOn the other hand, in the declarative Haskell example, we state that the sum of a list is simply the first element x plus the sum of the rest of the elements xs. This will then carry on recursively. As with all recursive functions, we require a base case to end the recursion, which is specified by sum [] = 0, which states to return 0 if the input to sum is an empty list. This ability to define the sum function twice with sum [] = 0 and sum (x:xs) = x + sum xs, is simply pattern matching.\nFinally, Haskell is statically typed, i.e., the types of the inputs and outputs of the function can be specified which in our case is sum :: [Float] -&gt; Float, which states that the function sum takes in a list of floats [Float] and returns a single float Float. This type declaration looks similar to how one might write f: X \\to Y mathematically for a function y = f(x), where x \\in X, and y\\in Y."
  },
  {
    "objectID": "posts/haskell-numerical.html#tensor-product-reduction-and-accumulation",
    "href": "posts/haskell-numerical.html#tensor-product-reduction-and-accumulation",
    "title": "Haskell for numerical computation?",
    "section": "Tensor product: reduction and accumulation",
    "text": "Tensor product: reduction and accumulation\nHere is a common example in quantum information/computation, where we have some quantum states that are complex matrices, and we want to perform a tensor product on them so that we can operate on them collectively.\n\nGiven a list of matrices, the function tensor_all returns the tensor product of all the elements of the list in sequence, e.g., given the list of matrices Ms = [A, B, C], tensor_all(Ms) should return their tensor product A\\otimes B\\otimes C.\n\n\nPythonHaskell\n\n\nimport numpy as np\n\ndef tensor_all(Ms):\n    tensor_prod = 1\n    for M in Ms:\n        tensor_prod = np.kron(tensor_prod, M)\n\n    return tensor_prod\n\n\nimport Numeric.LinearAlgebra\n\ntensor_all :: [Matrix C] -&gt; Matrix C\ntensor_all = foldl kronecker 1\n\n\n\nImperatively in the Python example, we loop through each element of the list and apply the tensor product (kronecker product np.kron) element-by-element with an accumulator, i.e., in the first loop we have (1) \\otimes A, in the second loop we have (1\\otimes A) \\otimes B, in the third loop we have (1\\otimes A \\otimes B) \\otimes C, and so on and so forth, where our accumulator tensor_prod is the value in the parenthesis, is initialized as 1, and is updated every loop.\nNote we have used the hmatrix library which is imported by import Numeric.LinearAlgebra. This documentation is a good resource for a comparison between hmatrix and Python’s numpy.\nIn Haskell, it should be obvious that we can perform a recursion like the case for sum. However, here we introduced the function foldl, which stands for “fold left”. foldl takes in a binary function (in this case kronecker), the initial accumulator (in this case 1), and a list (e.g., [a, b, c]). It then returns kronecker(kronecker(kronecker(1, a), b), c). There is also foldr, or “fold right”, which instead returns kronecker(a, kronecker(b, kronecker(c, 1))).\n\n\n\n\n\n\nPoint-free style programming\n\n\n\n\n\nNote that in the Haskell example, we could have written the function as tensor_all ms = foldl kronecker 1 ms, where the input to the function, the list of matrices ms, is specified. However, since ms is simply the input to the corresponding functions on both sides of the expression, we can omit it and simply write tensor_all = foldl kronecker 1. This is referred to as point-free style programming.\n\n\n\nIf instead, we want to “accumulate” the result of each loop into a list, i.e., we want the function to return [A, A \\otimes B, A\\otimes B\\otimes C], then, in the Python example we have to append the accumulator tensor_prod in each loop into a list. On the other hand, in the Haskell example, we can simply replace foldl with scanl.\n\nPythonHaskell\n\n\nimport numpy as np\n\ndef tensor_all(Ms):\n    tensor_prod = 1\n    tensor_prod_list = []\n    for M in Ms:\n        tensor_prod = np.kron(tensor_prod, M)\n        tensor_prod_list.append(tensor_prod)\n\n    return tensor_prod_list\n\n\nimport Numeric.LinearAlgebra\n\ntensor_all :: [Matrix C] -&gt; [Matrix C]\ntensor_all = scanl kronecker 1\n\n\n\nFolds and scans are part and parcel of a functional programmer, and are common in functional programming languages, as well as array programming languages. Typically, they are referred to as reduction and accumulation, e.g., in the array programming languages NumPy, Julia, and R, folds and scans are called reduce and accumulate.\n\n\n\n\n\nImage credit: code_report\n\n\nIf you would like to know more about array languages, and how they compare to functional languages like Haskell, I highly recommend the youtube channel code_report."
  },
  {
    "objectID": "posts/haskell-numerical.html#collision-model-currying-and-laziness",
    "href": "posts/haskell-numerical.html#collision-model-currying-and-laziness",
    "title": "Haskell for numerical computation?",
    "section": "Collision model: currying and laziness",
    "text": "Collision model: currying and laziness\nLet’s consider a less simple example to reveal more of Haskell’s features:\n\nA superconducting qubit \\rho (rho) is sensitive to heat, and is continuously undergoing a thermalization process. We can model this thermalization process with a collision model, where the qubit evolves via repeated “collision” with different qubits at each discrete time step for n (n) number of time steps. The collision is implemented by the function collision. We want to determine how the von Neumann entropy of \\rho, S(\\rho), changes in each collision up till n time steps. The calculation of the von Neumann entropy is implemented by the function entropy.\n\nFor completeness, the function collision performs the operation of \n\\rho_{t+1}\n= \\sum_i K_i \\rho_t K_i^\\dagger,\n where K_i are Kraus operators, and the function entropy computes \nS\\left(\\rho_t\\right) = -\\mathrm{Tr} \\left(\\rho_t \\log \\rho_t\\right).\n Therefore, what we want is simply the list of \\small[S(\\rho_0), S(\\rho_1), \\ldots, S(\\rho_{n-1})]\nThis might look complicated if one does not know quantum mechanics. However, if we assume that the collision and entropy calculations are done for us in the functions collision and entropy, then we simply want a list of n entropy values, i.e., entropy_list = [entropy(rho), entropy(collision(rho)), entropy(collision(collision(rho))),...].\n\nPythonHaskell\n\n\nimport scipy as sp\n\ndef collision(ks, rho):\n    return sum([k @ rho @ k.conj().T for k in ks])\n\ndef entropy(rho):\n    return - np.trace(rho @ sp.linalg.logm(rho))\n\ndef calc_entropies(ks, rho, n):\n    entropy_list = []\n    for _ in range(n):\n        rho_entropy = entropy(rho)\n        entropy_list.append(rho_entropy)\n        rho = collision(ks, rho)\n\n    return entropy_list\n\n\ncollision :: [Matrix C] -&gt; Matrix C -&gt; Matrix C\ncollision ks rho = sum [k &lt;&gt; rho &lt;&gt; tr k | k &lt;- ks]\n\nentropy :: Matrix C -&gt; Float\nentropy rho = - trace (rho &lt;&gt; logm rho)\n  where\n    logm = matFunc log\n\ncalc_entropies :: [Matrix C] -&gt; Matrix C -&gt; Int -&gt; [Float]\ncalc_entropies ks rho n = map entropy rho_list\n  where\n    rho_list = take n (iterate (collision ks) rho)\n\n\n\n\n\n\n\n\n\nExtra things to note for the Haskell example\n\n\n\n\n\n\nSimilar to Python, Haskell has list comprehension, which has a mathematical syntax as seen in the function collision, e.g.,\nexample = [p * q | p &lt;- ps, q &lt;- qs, p &gt;= 5, q /= 0]\nwould give a list of the set \\{pq\\ |\\ p \\in P,\\ q \\in Q,\\ p \\geq 5,\\ q \\neq 0 \\}.\nOn the other hand, Python’s list comprehension is less concise:\nexample = [p * q for p in ps for q in qs if p &gt;= 5 and q != 0]\nIn the entropy and calc_entropies functions, we have used the where keyword, which allows us to break a function into smaller constituents, e.g.,\nexampleFunc a b c = n / d\n  where\n    n = x + y\n    d = x - y\n    x = a + b + c\n    y = a * b * c\nAlternatively, we can also use the let and in keyword:\nexampleFunc a b c =\n  let\n    n = x + y\n    d = x - y\n    x = a + b + c\n    y = a * b * c\n  in\n    n / d\n\n\n\n\nThe Python example, being imperative, should be self-explanatory. Instead, we are interested in the Haskell example, specifically the calc_entropies function, which might look arcane if one does not know common functional programming functions such as map, take, and iterate. Let’s look at this line-by-line.\n\nCurrying\nThe very first line of the calc_entropies function is the type signature of:\n\n\\mathtt{calc\\_entropies :: [Matrix\\ C]\\ -&gt; Matrix\\ C\\ -&gt; Int\\ -&gt; [Float]}\n\nwhich states that the function calc_entropies takes in a list of complex matrix, a complex matrix, and an integer, and returns a list of floats. The list of complex matrix refers to the Kraus operators K_i used in the collision function, the complex matrix refers to rho or \\rho, the integer refers to n, while the list of floats refers to the output entropy_list.\nOne might notice that [Matrix C] -&gt; Matrix C -&gt; Int -&gt; [Float] doesn’t seem to make a clear distinction between inputs and outputs. This has to do with the concept of partial function application, or currying (also named after Haskell Curry).\nImplicitly, what’s happening is the following:\n\n\\mathtt{calc\\_entropies :: [Matrix\\ C]\\ -&gt;\\ } \\underbrace{\\mathtt{Matrix\\ C\\ -&gt; Int\\ -&gt; [Float]}}_\\text{function $f$}\n \nf\\ \\mathtt{:: Matrix\\ C\\ -&gt;\\ } \\underbrace{\\mathtt{Int\\ -&gt; [Float]}}_\\text{function $g$}\n \ng\\ \\mathtt{:: Int\\ -&gt; [Float]}\n\nwhere the function calc_entropies takes in [Matrix C] as its input, and returns a function f as its output. The function f then takes in Matrix C as the input and returns another function g as the output. Finally, the function g takes in Int as the input and returns [Float] as the final output. This means that functions in Haskell are indeed pure in that they only take in one input and one output, and any functions that appear to take in multiple inputs are in fact taking in only one input and returning a function that takes in also one input, and so on and so forth. This is called currying. Because of this, we can also apply the functions “partially”, for example:\nf a b = a + b\ng = f 5\nwhere we only provide one argument to f when it is expecting two, to create a new function g. This means that f 5 10 and g 10 would give the same output of 15. Currying or partial function application can be a powerful tool for the abstraction and expressiveness of your code.\n\n\n\n\n\n\nExample of the usefulness of currying\n\n\n\n\n\nUnitary operations are common in quantum mechanics, and are how quantum computers perform computations on quantum states or qubits. They are defined as follows: \n|\\psi'\\rangle = U |\\psi\\rangle.\n\nWe can define a function unitary_oper to implement this.\n\nPythonHaskell\n\n\ndef unitary_oper(U, ket):\n    return U @ ket\n\n\nunitary_oper :: Matrix C -&gt; Matrix C -&gt; Matrix C\nunitary_oper u ket = u &lt;&gt; ket\n\n\n\nThere are many common unitaries U used in quantum computations, referred to as quantum logic gates. For example there are the Pauli X gate and the Hadamard gate which are defined as \nX =\n\\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix},\\quad\nH = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{pmatrix}.\n\nCurrying or partial-function application allows us to easily define these new gate operations on top of the existing unitary_oper function:\npauli_x = unitary_oper (2&gt;&lt;2) [0, 1, 0, 1]\nhadamard = unitary_oper (cmap (/ sqrt 2) (2&gt;&lt;2) [1, 1, 1, -1])\nWe can then easily apply the Pauli X and Hadamard gates on a qubit |0\\rangle = [1, 0]^T with pauli_x (2&gt;&lt;1) [1, 0] and hadamard (2&gt;&lt;1) [1, 0]. This can make the code more concise and clearer.\n\n\n\n\n\nLazy evaluation\nMoving on to the function itself:\ncalc_entropies ks rho n = map entropy rho_list\n  where\n    rho_list = take n (iterate (collision ks) rho)\nThe first line features the function map, which is another part and parcel of the functional programmer. It simply apply a function to every element of a list:\n\n\\underbrace{\\mathtt{map\\quad entropy}}_\\text{apply entropy function to each element of}\\quad \\mathtt{rho\\_list}\n\nand has the corresponding Python code of\nentropy_list = [entropy(rho) for rho in rho_list]\n\n# or alternatively\nentropy_list = []\nfor rho in rho_list:\n    entropy_list.append(entropy(rho))\nFinally, in the last line we have\n\\mathtt{where\\quad rho\\_list} = \n\\underbrace{\\mathtt{take\\quad n}}_\\text{take first $n$ of}\\quad \\underbrace{\\mathtt{(iterate\\quad (collision\\quad ks)\\quad rho)}}_\\text{returns an infinite-sized list of [rho, collision(rho), collision(collision(rho)),...]}\n which gives rho_list = [rho, collision(rho), collision(collison(rho)),...] up till n number of elements in the list.\nAlso note that we can reduce the number of parenthesis by using $ instead, e.g., rho_list = take n $ iterate (collision ks) rho.\nIt might be quite surprising that the iterate function returns an infinite-sized list. For example, iterate add1 10 gives [11, 12, 13, 14, 15, ...] to infinity, where the function add1 is applied on 10 ad infinitum. If we were to write the corresponding Python code for rho_list = take n (iterate (collision ks) rho), it might look something like\nrhos = []\nwhile True:\n    rhos.append(rho)\n    rho = collision(ks, rho)\n\nrho_list = rhos[:n]\nwhich runs forever and result in a list of infinite size. However, this is not a problem in Haskell, thanks to the fact that Haskell is lazy-evaluated. This means that Haskell only evaluate values when required, and so since we only require the first n elements of the infinite list, Haskell only evaluate that first n elements.\nIf this Python code is lazy-evaluated like in Haskell, one might imagine that the program looks ahead and saw that it only require the first n elements as stated in rho_list = rhos[:n], and so it stops the while True loop after only running n times."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Email: contact@jianwei.simplelogin.com\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/why-fedora.html",
    "href": "posts/why-fedora.html",
    "title": "Why I recommend Fedora for new Linux users",
    "section": "",
    "text": "Here, I provide a list of reasons on why I think Fedora is a good starting point for new Linux users, which comes down to a single advantage: good defaults.\n\nSemi-rolling release\nWhen you use your computer, here’s what you might expect:\n\nYou want all your programs to be updated to the latest versions.\nYou want the stuffs running in the background to be updated to the latest versions.\nYou don’t want major changes to your user experience or to how things are done.\n\nThis is reasonable; you want frequent updates because you want the latest features, or security and bug fixes. But at the same time, you don’t want your desktop to completely change after an update (think Windows 7 to 8, or 10 to 11).\nFedora’s semi-rolling release cycle abide by these: you get frequent updates (including kernel updates), but some packages like the desktop environment GNOME, or major software decisions (like moving to a new technology), are only updated in a major version release that the user must initiate.\n\n\nWayland\nMost Linux distros use X11 as its windowing system (a program that handles how windows are drawn on your screen). However, X11 does not have GUI isolation, allowing different programs to interact with one another. A malicious program can then record your screen, or log the keys you typed in your browser while you are logging in to your bank.\nWayland is the successor to X11, and is developed with security in mind, isolating each window from one another.\n\nUnlike X, the Wayland input stack doesn’t allow applications to snoop on the input of other programs (preserving confidentiality), to generate input events that appear to come from the user (preserving input integrity), or to capture all the input events to the exclusion of the user’s application (preserving availability).\n— Jake Edge, LWN.net\n\n\n\nSwap on ZRAM\nLinux systems offers a “swap” space, where memory on the RAM can be copied to disk space, freeing the RAM in the process. Swap is an important part of a Linux operating system. However, sensitive information stored on RAM can be copied to disk, risking potential security issues. ZRAM solves these problems by having the swap space remain in the RAM with compression, ensuring that no sensitive information will be stored on disk.\n\n\nBtrfs\nMost Linux distros have traditionally used the ext4 filesystem. btrfs is a newer file system that offers advantages over ext4:\n\nCompression leads to less disk space used.\nCompression also extends the lifespan of flash-based devices (SSDs, etc.).\nLess likely to have data corruption.\nProvides snapshots that allows rollback when system breaks.\n\n\nFor laptop and workstation installs of Fedora, we want to provide file system features to users in a transparent fashion. We want to add new features, while reducing the amount of expertise needed to deal with situations like running out of disk space. Btrfs is well adapted to this role by design philosophy, let’s make it the default.\n…\nThe Btrfs community has users that have been using it for most of the past decade at scale. It’s been the default on openSUSE (and SUSE Linux Enterprise) since 2014, and Facebook has been using it for all their OS and data volumes, in their data centers, for almost as long. Btrfs is a mature, well-understood, and battle-tested file system, used on both desktop/container and server/cloud use-cases.\n— Fedora Project wiki, Btrfs changes proposal\n\n\n\nSELinux and firewalld\nSecurity Enhanced Linux (SELinux) provides Mandatory Access Control (MAC).\n\nIn computer security, mandatory access control (MAC) refers to a type of access control by which the operating system or database constrains the ability of a subject or initiator to access or generally perform some sort of operation on an object or target.\n— Wikipedia\n\n\nSecurity Enhanced Linux (SELinux) provides an additional layer of system security. SELinux fundamentally answers the question: May &lt;subject&gt; do &lt;action&gt; to &lt;object&gt;?, for example: May a web server access files in users’ home directories?\n— Fedora docs\n\nFedora comes with good default policies for SELinux, as well as good default firewall rules with firewalld.\n\n\nFlatpak enabled\nFlatpak is supposed to be a new and universal way of installing programs. Flatpak is now available on almost all major distros and is also the most popular way of installing programs on the Steam Deck.\nUbuntu on the other hand, while often recommended to beginners, decided to drop Flatpak in favor of snap, which is their own alternative that is much less popular. snap packages masquerade as traditional packages, which can cause confusion for new users. They also encourage users to sign up an Ubuntu One account just to install snap packages without root.\n\n\nSpeed of adopting new technologies\nFedora tends to adopt new technologies at a fast pace, such as Wayland, swap on ZRAM, Flatpak, PipeWire, btrfs etc. New technologies tends to offer improvements in terms of usability and security.\nFor example, Fedora adopted Wayland in 2016, but Ubuntu only done so in 2021. Even Red Hat Enterprise Linux (RHEL) and Debian adopted Wayland in 2019, earlier than Ubuntu, despite both being extremely conservative distros.\n\n\nWhat’s next?\nIf you just want a usable system with no interest in learning more about the Linux ecosystem, then I believe Fedora is the ideal choice. However, I do not encourage people to stay on Fedora for the rest of their Linux journey. Rather, one should try out different Linux distros and explore alternatives to learn more about the Linux ecosystem.\nEventually, as you gain more understanding of the Linux ecosystem, you will develop your own opinions on things. For example, you might be against Flatpak, or perhaps you might think that systemd has major problems. Regardless, the greatest strength of the Linux ecosystem is the power to make your own choices on what software and technologies you prefer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/life-conspiracy.html",
    "href": "posts/life-conspiracy.html",
    "title": "Life as an entropic conspiracy",
    "section": "",
    "text": "“What is the meaning of life?”\n\nThis question has become quite a cliché, with an equally cliché answer of:\n\n“It’s whatever you make it out to be!”\n\nthat has become the norm.\nI believe the reason why such a cop-out answer is the norm is because deep down, we all understand that life has no intrinsic objective meaning1. In fact, given that the prerequisite for the concept of “meaning” to exist in the first place is the existence of life itself, this question seems rather like a trick question.\n1 Sans whatever religious/spiritual beliefs that one might have.2 e.g., the aforementioned religious/spiritual beliefs.Similar to how a system cannot demonstrate its own consistency in Gödel’s second incompleteness theorem, perhaps one must require a framework that is external to life itself to apply “meaning” for life’s existence, before life’s existence2.\nNevertheless, by replacing the quite human term of “meaning” with a more mechanistic term like “purpose”, we might be able to answer more reasonably the resulting question of:\n\n“What is the purpose of life?”"
  },
  {
    "objectID": "posts/life-conspiracy.html#self-organization-order-out-of-chaos",
    "href": "posts/life-conspiracy.html#self-organization-order-out-of-chaos",
    "title": "Life as an entropic conspiracy",
    "section": "Self-organization — order out of chaos",
    "text": "Self-organization — order out of chaos\nSelf-organization of complex structures in the presence of an external energy source have been demonstrated both experimentally and computationally. In a computational simulation involving 20 particles in a simplified chemical space, complex bonding patterns emerged under resonance with a periodic drive, increasing the rate of work absorption and heat dissipation (Kachman, Owen, and England 2017).\n\n\n\n\n\nReprinted from Kondepudi, Kay, and Dixon (2015)\n\n\nSetup and result of self-organizing conducting beads.\nExperimentally, conducting beads 4mm in diameter immersed in oil in a petri dish were shown to self-organize to a complex, tree-like structure when subjected to a high voltage. This occured at a critical point and was accompanied by a spike in entropy dissipation (Kondepudi, Kay, and Dixon 2015). Silver nanorods were also observed to self-organized in the presence of a doughnut-shaped laser source, with resulting structures depending on the wavelengths and polarization of the laser source (Ito et al. 2013).\nLikewise, by considering UVC light of 230nm to 270nm from the Sun as the non-equilibrium energy source (a range of wavelengths that were most intense in the early Earth), it was demonstrated that RNA polymers could proliferate via photochemical reactions, and that DNA polymers would denature from absorption and dissipation at these UVC wavelengths. These are crucial steps for self-replication without enzymes (Karo Michaelian 2017; Karo Michaelian and Padilla 2019).\nHomochirality in DNA and RNA was also shown to be achieved via photochemical reactions with UVC wavelengths light (Karo Michaelian 2018). Correlations also exist between the maximum absorbance wavelengths of early life’s nucleobases and amino acids, and UVC wavelengths (K. Michaelian and Simeonov 2015).\nThese findings suggest that in the RNA world during early Earth era, RNA or pre-RNA polymers may have self-organized or adapted to efficeintly absorb and dissipate UVC solar spectrum, a prime example of dissipative adaptation in the emergence of life."
  },
  {
    "objectID": "posts/life-conspiracy.html#self-replication-the-more-the-merrier",
    "href": "posts/life-conspiracy.html#self-replication-the-more-the-merrier",
    "title": "Life as an entropic conspiracy",
    "section": "Self-replication — the more the merrier",
    "text": "Self-replication — the more the merrier\nClearly, the more dissipators there are in a system, the greater the overall entropic dissipation. It’s therefore unsurprising that complex systems can self-replicate to increase entropy dissipation.\nIn fact, even the act of self-replication itself can serves to dissipate heat, e.g., it was shown that E. coli bacteria dissipate at least 5 times as much heat during replication compared to individual heat dissipation (England 2013).\nThe process of DNA replication (copolymerisation) also dissipates entropy depending on the information entropy of the DNA polymer. Information creation decreases the Shannon entropy of the polymer and generates mutual information with the original. In the process, entropy is dissipated to the environment as quantified by the affinity per copied nucleotide (Andrieux and Gaspard 2008).\nIn general, various dissipative systems exhibit self-replication behaviors. For example, computer simulations of colloidal clusters show that the exponential growth rates of self-replication varies with the energy landscape of the system (Zeravcic and Brenner 2014).\nThree-dimensional vortices also self-replicates in rotating shear flows, as demonstrated in a numerical study (Marcus et al. 2013). This was also proposed to be how planetoids form in protoplanetary disks (Barranco and Marcus 2005), whereby the larger system can replicate smaller dissipative systems in it, or eventually split into a copy or more of itself, i.e., disk fragmentation that forms binary star systems (Offner et al. 2010).\nGiven these observations, one might hypothesize that early organic compounds evolved the ability to self-replicate as a means to ensure the constant presence of entropy dissipating structures, driving the emergence and evolution of life."
  },
  {
    "objectID": "posts/life-conspiracy.html#adaptation-resonance-with-the-environment",
    "href": "posts/life-conspiracy.html#adaptation-resonance-with-the-environment",
    "title": "Life as an entropic conspiracy",
    "section": "Adaptation — resonance with the environment",
    "text": "Adaptation — resonance with the environment\nIf life indeed self-organizes to more efficiently absorb external energy sources and dissipate heat, we would expect it to adapt or change its structures in response to changes in those energy sources over time.\n\n\n\n\n\nReprinted from K. Michaelian and Simeonov (2015)\n\n\nCorrelations between solar spectrum and the emergence of organic compounds and cells in the five different periods. As Earth’s conditions changes, more complex organic compounds evolved to suit these conditions. The general trends are a shift from UVC absorbing to visible spectrum absorbing compounds, and a shift to oxygen based respiration.\nThis seems to be evident throughout life’s history, where its evolution has underwent avalanche dynamics with quick and sudden changes in complexity rather than smooth and gradual evolution (Paczuski, Maslov, and Bak 1996). Some even modeled these sudden transitions of evolution as a Big Bang model with “evolutionary inflation” akin to the cosmological inflation model (Koonin 2007). Simulated artificial life have also demonstrated how the fitness or how the population adapts to the environment, can undergo multiple discontinuous jumps (Adami 1995).\nCorrelating Earth’s historical environmental changes (like solar spectrum, temperature, and atmospheric compositions) with organic compounds’ emergence and evolution shows that life has consistently adapted to better survive and absorb energy according to the prevailing condition in that period (K. Michaelian and Simeonov 2015).\nAn interesting example is the case of adenosine triphosphate (ATP) synthase, an enzyme that lies along the cell membrane of a cell that breaks down ATP to adenosine diphosphate (ADP) and a phosphate. This breakage releases energy that powers the cell.\nUsing a kinetic model of the ATP synthase process reveals that both its entropy production and the Shannon entropy of the transitional states, are maximized at a catalytic dwell angle3 consistent with the actual ATP synthase’s angle, indicating that the ATP synthase evolved to optimize entropy production (Dewar, Juretić, and Županović 2006).\n3 The ATP synthase rotates in angles of 120^{\\circ} for each ATP to ADP + P_i reaction. However, at the moment of breakage of ATP bonds to ADP + P_i, the synthase will pause for a short amount of time of around 2ms (Ueno et al. 2005). The angle at which this pause happens is known as the catalytic dwell.These might findings suggest that the process of self-organization to an external energy source is an ongoing, dynamic phenomenon that continues in response to changing conditions."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\nDate\nTitle\nReading Time\n\n\n\n\nMay 1, 2024\nLife as an entropic conspiracy\n8 min\n\n\nSep 5, 2023\nHaskell for numerical computation?\n15 min\n\n\nApr 1, 2023\nWhy I recommend Fedora for new Linux users\n5 min\n\n\n\n\nNo matching items\n\n Back to top"
  }
]