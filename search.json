[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jian Wei CHEONG",
    "section": "",
    "text": "ORCID\n  \n  \n    \n     Research group\n  \n\n      \nHi, I am Jian Wei, a Physics research fellow in Nanyang Technological University, Singapore.\nMy research interests are in the fields of quantum information and quantum thermodynamics.\nI play with Linux and do programming for fun.\nI believe in the efficiency of workflow. Everything should be as painless as possible, doing more with less.\n\n    \n    \n  \n\n\n\n\nRecent posts\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\nThe Betty Hill Star Map: Where are my aliens from?\n\n\n20 min\n\n\n\n\n\n\nJun 8, 2025\n\n\nUseful tools for the academic\n\n\n7 min\n\n\n\n\n\n\nJan 11, 2025\n\n\nAn honest explanation of quantum computing\n\n\n11 min\n\n\n\n\n\n\nMay 1, 2024\n\n\nLife as an entropic conspiracy\n\n\n8 min\n\n\n\n\n\n\nSep 5, 2023\n\n\nHaskell for numerical computation?\n\n\n15 min\n\n\n\n\n\n\nApr 1, 2023\n\n\nWhy I recommend Fedora for new Linux users\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n  \n\n\nsee more\n\n\n\nList of publications\n\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Non-Markovian refrigeration and heat flow in the quantum switch, Physical Review A, 110(2), 022220 (2024).\nL. Y. Chew, A. Pradana, L. He, and J. W. Cheong, Stochastic thermodynamics of finite-tape information ratchet, European Physical Journal Special Topics (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Effects of non-Markovianity on daemonic ergotropy in the quantum switch, Physical Review A, 108(1), 012201 (2023).\nL. He, J. W. Cheong, A. Pradana, and L. Y. Chew, Effects of correlation in an information ratchet with finite tape, Physical Review E, 107(2), 024130 (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Communication advantage of quantum compositions of channels from non-Markovianity, Physical Review A, 106(5), 052410 (2022).\nL. He, A. Pradana, J. W. Cheong, and L. Y. Chew, Information processing second law for an information ratchet with finite tape, Physical Review E, 105(5), 054131 (2022).\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\nThe Betty Hill Star Map: Where are my aliens from?\n\n\n20 min\n\n\n\n\n\n\nJun 8, 2025\n\n\nUseful tools for the academic\n\n\n7 min\n\n\n\n\n\n\nJan 11, 2025\n\n\nAn honest explanation of quantum computing\n\n\n11 min\n\n\n\n\n\n\nMay 1, 2024\n\n\nLife as an entropic conspiracy\n\n\n8 min\n\n\n\n\n\n\nSep 5, 2023\n\n\nHaskell for numerical computation?\n\n\n15 min\n\n\n\n\n\n\nApr 1, 2023\n\n\nWhy I recommend Fedora for new Linux users\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/tools-academic.html",
    "href": "posts/tools-academic.html",
    "title": "Useful tools for the academic",
    "section": "",
    "text": "Throughout my time in academia, there are many instances where I witnessed the quickness of my colleagues in picking up and gaining proficiency in some new skills or tools that serves to enhance their work.\nThese not only include new domain knowledge, e.g., learning machine learning to solve a niche problem, but also tools that aids in their work. The regnant LaTeX need no introduction, and picking up C/C++ to speed up computation or learning Git for collaborative work are quite common.\nHowever, it seems to me that many are satisfied in their “local minima”, and do not have the time or effort to look for new tools that might enhance their work. Therefore, here I introduce some tools that I found to be useful that I think more academics should use1."
  },
  {
    "objectID": "posts/tools-academic.html#pandoc",
    "href": "posts/tools-academic.html#pandoc",
    "title": "Useful tools for the academic",
    "section": "Pandoc",
    "text": "Pandoc\nPandoc is a universal document converter. It can convert MS Word to and from LaTeX, an HTML page to EPUB, or MS PowerPoint to HTML, and many more! However, the most popular feature of the universal converter is the ability to write in Markdown and convert it to a document or a presentation slide.\nHere, we write in simple Markdown and convert to a PDF document via LaTeX2.\n2 We can also convert to presentation slides via LaTeX beamer (or reveal.js): \n\n\nMarkdown is much easier to write as compared to LaTeX.\n\n\nFor a long time I have used Pandoc for all my write-ups, notes, and presentation slides due to the simplicity of writing Markdown over LaTeX, MS Word, and MS PowerPoint3, but I have since moved to better tools below.\n3 Combined with wrangling data in Python/Julia/R over MS Excel, I have completely ditched MS Office."
  },
  {
    "objectID": "posts/tools-academic.html#quarto",
    "href": "posts/tools-academic.html#quarto",
    "title": "Useful tools for the academic",
    "section": "Quarto",
    "text": "Quarto\nComing from the lovely R community and building on top of Pandoc, Quarto is the successor to R Markdown, an attempt at literate programming. R Markdown does the same as Pandoc, allowing one to write in Markdown and outputs to documents and presentations etc., with the exception that it also allows one to run and output R code inside the document. Quarto extends this functionality to include Python, Julia, and Observable in addition to R.\n\n\n\nReprinted from Quarto’s tutorial.\n\n\nThis functionality allows us to mix computation with its documentations, explanations, or mathematics in the same document.\n\n\n\nThe output of the code appears in the output document!\n\n\nThis allows for reproducible research where the codes and outputs are contained in the same research write-up, and is perfect for cases where one wants to share technical reports complete with implementation details to a colleague.\nBut this is not all! Even without using the ability to run code in your documents, Quarto can still do a lot more, e.g., writing books, creating dashboards for data science, or creating web apps to interact with data4. One can also publish to Quarto Pub to share their Quarto documents easily.\n4 In fact, this very website is written in Quarto!"
  },
  {
    "objectID": "posts/tools-academic.html#typst",
    "href": "posts/tools-academic.html#typst",
    "title": "Useful tools for the academic",
    "section": "Typst",
    "text": "Typst\nTypst is a modern typesetting system that aims to be an alternative to LaTeX. Fast, easy, sensible, with an actual scripting language, and actively developed, it has completely replaced LaTeX for me except for papers that are submitted for publication5. I even made a conference poster recently with Typst6.\n5 As far as I am aware, IJIMAI is the only journal that accept Typst at this point of time.6 The package peace-of-posters can be used to easily create posters. Not only does Typst has a simple Markdown-like syntax, it is also extremely fast, allowing almost instant live previews while typing.\n\n\nVideo\nLive preview in Typst.\n\n\nTypst also has an active package ecosystem, from drawing diagrams with CeTZ, creating presentation slides with Touying, drawing quantum circuits with Quill, to plotting with Lilaq, Typst covers my entire workflow. My research notes that I share with my colleagues are written in Typst, I make diagrams in Typst, and I make worksheets for students in Typst.\nTypst has also gained enough popularity that Pandoc and Quarto now supports it as a PDF backend in place of LaTeX. It even comes preinstalled in Quarto. Unlike LaTeX, there are almost no wait time in compiling a document, and error messages are not cryptic, making Typst a good PDF backend for Quarto7.\n7 Take a look at the talks “Never again in outer par mode” from Posit Conference 2023 and “Styling Quarto PDFs with Typst” from SatRdays London 2024, where they discusses the advantages of using Typst over LaTeX in Quarto"
  },
  {
    "objectID": "posts/tools-academic.html#an-rss-feed-reader",
    "href": "posts/tools-academic.html#an-rss-feed-reader",
    "title": "Useful tools for the academic",
    "section": "An RSS feed reader",
    "text": "An RSS feed reader\nYou most likely have come across this little orange icon  in your time on the internet. In fact, there is one at the top right of this very site (though it’s not orange). This is RSS (Really Simple Syndication), a standardized format to aggregate news or contents into a single feed.\nImagine having a single feed where you can read articles from multiple sources such as news websites, right when they are published. This is not an uncommon concept as some smartphones such as the iPhone provides such features with their news app. So how is this useful for academics?\nWell, turns out journals typically provide a link to subscribe to their RSS feed too. One can then easily keep up with current research in journals relevant to their fields by aggregating them into a single RSS feed reader.\n\n\n\nThe RSS feed reader Newsraft showing new published papers in Physical Review Letters.\n\n\nI check the feed every day, reading through the titles, and then the abstracts for titles that caught my attention. If I find the paper interesting or relevant, I can easily open the paper on a browser with a single hotkey to download or add it to my bibliography manager.\nI am subscribed to 11 journals, totaling to around 50 to 100 new papers everyday (arXiv takes up the bulk of this number), a number that is small enough that it’s not a hassle to go through the feed every day8.\n8 I’m also subscribed to some news site and even some YouTube channels, eliminating the need for an account.\n\n\nReading the abstract of an arXiv paper in Newsraft.\n\n\nGiven how important keeping up with current developments in the field is for a researcher, an RSS feed reader (and there are many) makes the aggravating task of checking for new papers effortless, and greatly increases one’s exposure to current research."
  },
  {
    "objectID": "posts/tools-academic.html#marimo-over-jupyter",
    "href": "posts/tools-academic.html#marimo-over-jupyter",
    "title": "Useful tools for the academic",
    "section": "Marimo over Jupyter",
    "text": "Marimo over Jupyter\nHere’s a bonus one.\nI take the somewhat popular yet controversial opinion that Jupyter notebooks are a scourge to scientific research. There are some annoying issues, such as how it’s saved as JSON instead of plain Python files, and how it teaches bad programming practices and habits to scientists. The latter can easily compound with carelessness to lead to invisible bugs and mistakes that produce wrong results, which might even end up in published research9!\n9 I haven’t encounter many researchers who actually write unit tests for their spaghetti code function.But much more troubling is the fact that Jupyter notebooks increase the chances of making these mistakes due to the creation of hidden states. In fact, a study showed that out of over 800 000+ valid notebooks on GitHub, only ~24% are executed without errors, and only ~4% produced the same results (Pimentel et al. 2019).\nPerhaps this warrant a separate post10, but in short, if one wants to present some results with their corresponding codes, use Quarto, and if one really require a notebook environment, use Marimo, an alternative to Jupyter notebooks (only for Python).\n10 For now, take a look at this talk from JupyterCon 2018 to understand some of the problems with Jupyter notebooks, as well as this FAQ from Marimo.The main difference between Marimo notebooks and Jupyter notebooks is the fact that the former is reactive. What this means is that Marimo notebooks always run in the correct order and always rerun the entire notebook on edits, eliminating the hidden states that plagues Jupyter notebooks.\nIf a code cell defining a variable is deleted, but the variable is used elsewhere, an error will be thrown. On the other hand, Jupyter notebooks continue to sneakily use that variable even when the programmer meant to remove it from the program.\nA researcher should be aware of how certain shortcomings of their tools can be detrimental to their work, and should be open to finding alternatives or engage in self-improvement to eliminate them to minimize bad research outputs.\nFor example, learning to write properly unit tested functions instead of spaghetti codes, or learning to use type hinting in Python (which has been around since version 3.5) to pair with type checkers to minimize invisible bugs caused by carelessness that can lead to wrong published results."
  },
  {
    "objectID": "posts/life-conspiracy.html",
    "href": "posts/life-conspiracy.html",
    "title": "Life as an entropic conspiracy",
    "section": "",
    "text": "“What is the meaning of life?”\n\nThis question has become quite a cliché, with an equally cliché answer of:\n\n“It’s whatever you make it out to be!”\n\nthat has become the norm.\nI believe the reason why such a cop-out answer is the norm is because deep down, we all understand that life has no intrinsic objective meaning1. In fact, given that the prerequisite for the concept of “meaning” to exist in the first place is the existence of life itself, this question seems rather like a trick question.\n1 Sans whatever religious/spiritual beliefs that one might have.2 e.g., the aforementioned religious/spiritual beliefs.Similar to how a system cannot demonstrate its own consistency in Gödel’s second incompleteness theorem, perhaps one must require a framework that is external to life itself to apply “meaning” for life’s existence, before life’s existence2.\nNevertheless, by replacing the quite human term of “meaning” with a more mechanistic term like “purpose”, we might be able to answer more reasonably the resulting question of:\n\n“What is the purpose of life?”3\n3 The term “purpose” might still be quite “agentic”, in the sense that it might mislead one into thinking that life has some agency over its own evolution based on some sense of purpose that it has. I assure that this is not what this post is about."
  },
  {
    "objectID": "posts/life-conspiracy.html#self-organization-order-out-of-chaos",
    "href": "posts/life-conspiracy.html#self-organization-order-out-of-chaos",
    "title": "Life as an entropic conspiracy",
    "section": "Self-organization — order out of chaos",
    "text": "Self-organization — order out of chaos\nSelf-organization of complex structures in the presence of an external energy source have been demonstrated both experimentally and computationally. In a computational simulation involving 20 particles in a simplified chemical space, complex bonding patterns emerged under resonance with a periodic drive, increasing the rate of work absorption and heat dissipation (Kachman, Owen, and England 2017).\n\n\n\n\n\nReprinted from Kondepudi, Kay, and Dixon (2015)\n\n\nSetup and result of self-organizing conducting beads.\nExperimentally, conducting beads 4mm in diameter immersed in oil in a petri dish were shown to self-organize to a complex, tree-like structure when subjected to a high voltage. This occurred at a critical point and was accompanied by a spike in entropy dissipation (Kondepudi, Kay, and Dixon 2015). Silver nanorods were also observed to self-organized in the presence of a doughnut-shaped laser source, with resulting structures depending on the wavelengths and polarization of the laser source (Ito et al. 2013).\nLikewise, by considering UVC light of 230nm to 270nm from the Sun as the non-equilibrium energy source (a range of wavelengths that were most intense in the early Earth), it was demonstrated that RNA polymers could proliferate via photochemical reactions, and that DNA polymers would denature from absorption and dissipation at these UVC wavelengths. These are crucial steps for self-replication without enzymes (Karo Michaelian 2017; Karo Michaelian and Padilla 2019).\nHomochirality in DNA and RNA was also shown to be achieved via photochemical reactions with UVC wavelengths light (Karo Michaelian 2018). Correlations also exist between the maximum absorbance wavelengths of early life’s nucleobases and amino acids, and UVC wavelengths (K. Michaelian and Simeonov 2015).\nThese findings suggest that in the RNA world during early Earth era, RNA or pre-RNA polymers may have self-organized or adapted to efficeintly absorb and dissipate UVC solar spectrum, a prime example of dissipative adaptation in the emergence of life."
  },
  {
    "objectID": "posts/life-conspiracy.html#self-replication-the-more-the-merrier",
    "href": "posts/life-conspiracy.html#self-replication-the-more-the-merrier",
    "title": "Life as an entropic conspiracy",
    "section": "Self-replication — the more the merrier",
    "text": "Self-replication — the more the merrier\nClearly, the more dissipators there are in a system, the greater the overall entropic dissipation. It’s therefore unsurprising that complex systems can self-replicate to increase entropy dissipation.\nIn fact, even the act of self-replication itself can serves to dissipate heat, e.g., it was shown that E. coli bacteria dissipate at least 5 times as much heat during replication compared to individual heat dissipation (England 2013).\nThe process of DNA replication (copolymerisation) also dissipates entropy depending on the information entropy of the DNA polymer. Information creation decreases the Shannon entropy of the polymer and generates mutual information with the original. In the process, entropy is dissipated to the environment as quantified by the affinity per copied nucleotide (Andrieux and Gaspard 2008).\nIn general, various dissipative systems exhibit self-replication behaviors. For example, computer simulations of colloidal clusters show that the exponential growth rates of self-replication varies with the energy landscape of the system (Zeravcic and Brenner 2014).\nThree-dimensional vortices also self-replicates in rotating shear flows, as demonstrated in a numerical study (Marcus et al. 2013). This was also proposed to be how planetoids form in protoplanetary disks (Barranco and Marcus 2005), whereby the larger system can replicate smaller dissipative systems in it, or eventually split into a copy or more of itself, i.e., disk fragmentation that forms binary star systems (Offner et al. 2010).\nGiven these observations, one might hypothesize that early organic compounds evolved the ability to self-replicate as a means to ensure the constant presence of entropy dissipating structures, driving the emergence and evolution of life."
  },
  {
    "objectID": "posts/life-conspiracy.html#adaptation-resonance-with-the-environment",
    "href": "posts/life-conspiracy.html#adaptation-resonance-with-the-environment",
    "title": "Life as an entropic conspiracy",
    "section": "Adaptation — resonance with the environment",
    "text": "Adaptation — resonance with the environment\nIf life indeed self-organizes to more efficiently absorb external energy sources and dissipate heat, we would expect it to adapt or change its structures in response to changes in those energy sources over time.\n\n\n\n\n\nReprinted from K. Michaelian and Simeonov (2015)\n\n\nCorrelations between solar spectrum and the emergence of organic compounds and cells in the five different periods. As Earth’s conditions changes, more complex organic compounds evolved to suit these conditions. The general trends are a shift from UVC absorbing to visible spectrum absorbing compounds, and a shift to oxygen based respiration.\nThis seems to be evident throughout life’s history, where its evolution has underwent avalanche dynamics with quick and sudden changes in complexity rather than smooth and gradual evolution (Paczuski, Maslov, and Bak 1996). Some even modeled these sudden transitions of evolution as a Big Bang model with “evolutionary inflation” akin to the cosmological inflation model (Koonin 2007). Simulated artificial life have also demonstrated how the fitness or how the population adapts to the environment, can undergo multiple discontinuous jumps (Adami 1995).\nCorrelating Earth’s historical environmental changes (like solar spectrum, temperature, and atmospheric compositions) with organic compounds’ emergence and evolution shows that life has consistently adapted to better survive and absorb energy according to the prevailing condition in that period (K. Michaelian and Simeonov 2015).\nAn interesting example is the case of adenosine triphosphate (ATP) synthase, an enzyme that lies along the cell membrane of a cell that breaks down ATP to adenosine diphosphate (ADP) and a phosphate. This breakage releases energy that powers the cell.\nUsing a kinetic model of the ATP synthase process reveals that both its entropy production and the Shannon entropy of the transitional states, are maximized at a catalytic dwell angle4 consistent with the actual ATP synthase’s angle, indicating that the ATP synthase evolved to optimize entropy production (Dewar, Juretić, and Županović 2006).\n4 The ATP synthase rotates in angles of 120^{\\circ} for each ATP to ADP + P_i reaction. However, at the moment of breakage of ATP bonds to ADP + P_i, the synthase will pause for a short amount of time of around 2ms (Ueno et al. 2005). The angle at which this pause happens is known as the catalytic dwell.5 One should keep in mind that none of these results were shown for complex life such as us humans, and one should not have the illusion that this hints at some sort of “Lamarckism”.These findings suggest that the process of self-organization to an external energy source is an ongoing, dynamic phenomenon that continues in response to changing conditions.5"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Email: contact@jianwei.simplelogin.com\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/haskell-numerical.html",
    "href": "posts/haskell-numerical.html",
    "title": "Haskell for numerical computation?",
    "section": "",
    "text": "Recently, I have been experimenting with Haskell which I find to be quite enjoyable to program in, thanks to its unique programming paradigm. So much so that I am considering using it for numerical computations in my research.\nSide-effects refers to having an effect on the “outside world” such as printing a string to terminal or writing to a file etc. Of course, we do want some side-effects for the program to do anything useful. In Haskell, side-effects are handled by monads, which is a beast of a topic on its own. Monads allow the pure and impure (side-effects) to be clearly separated in Haskell.\nHaskell (named after logician Haskell Curry) is a purely functional programming language in the mathematical sense. Every function in Haskell simply takes an input and returns an output—nothing more, nothing less. It cannot mutate variables, and it cannot produce side-effects.\nPurely functional programming languages are typically declarative, i.e., one does not write step-by-step instructions like imperative languages such as Python, C, C++, etc. For example, there are no loops in Haskell; one has to rely on recursion or functions such as ‘fold’ and ‘scan’.\nThe two main reasons for my consideration to use it for numerical computations are:\n\nConciseness: Haskell’s declarative style greatly increase its conciseness. Prototyping and experimentation are frequent in computational research. Concise codes can reduce this programming time, and allow faster prototyping.\nSafety and correctness: A small bug in numerical computational would not raise any error. Instead, it manifests in computed results or plots, masquerading as valid or even novel results. If these bugs are not identified, one might publish a paper with faux results.\n\nHaskell’s static typing can serve as an additional check to reduce bugs.\nIts declarative approach means that codes are written in easily verifiable expressions, like in mathematics, rather than imperative statements that are often prone to bugs or human error.\nImmutability and the absence of side-effects in functions can also reduce hidden bugs. For instance, the risk of a function modifying a variable without the coder’s knowledge is mitigated, and parallel computations are less likely to produce incorrect results due to variable mutations in multiple threads.\n\n\nI am neither a software developer nor am I computer science trained, and so some technical aspects of the language might escape me. Therefore, in this article, we will only examine some simple examples to compare traditional imperative languages like Python with purely functional Haskell."
  },
  {
    "objectID": "posts/haskell-numerical.html#sum-recursion",
    "href": "posts/haskell-numerical.html#sum-recursion",
    "title": "Haskell for numerical computation?",
    "section": "Sum: recursion",
    "text": "Sum: recursion\nIgnoring the fact that the function sum is built-in in both Python and Haskell, let’s see how one might write a sum function that sum all the elements in a list.\n\nPythonHaskell\n\n\ndef sum(xs):\n    sum_all = 0\n    for x in xs:\n        sum_all += x\n    return sum_all\n\n\nsum :: [Float] -&gt; Float\nsum [] = 0\nsum (x:xs) = x + sum xs\n\n\n\nIn the imperative Python example, we tell the program what to do step-by-step: first initialize an accumulator sum_all to 0, then loop through each elements in the list and add them to the accumulator.\nOne might notice that functions are applied without parenthesis, i.e., applying a function foo on a variable x, is typically foo(x), but would be foo x in Haskell. The reason for this has something to do with the concept of “currying” which we will see later.\nOn the other hand, in the declarative Haskell example, we state that the sum of a list is simply the first element x plus the sum of the rest of the elements xs. This will then carry on recursively. As with all recursive functions, we require a base case to end the recursion, which is specified by sum [] = 0, which states to return 0 if the input to sum is an empty list. This ability to define the sum function twice with sum [] = 0 and sum (x:xs) = x + sum xs, is simply pattern matching.\nFinally, Haskell is statically typed, i.e., the types of the inputs and outputs of the function can be specified which in our case is sum :: [Float] -&gt; Float, which states that the function sum takes in a list of floats [Float] and returns a single float Float. This type declaration looks similar to how one might write f: X \\to Y mathematically for a function y = f(x), where x \\in X, and y\\in Y."
  },
  {
    "objectID": "posts/haskell-numerical.html#tensor-product-reduction-and-accumulation",
    "href": "posts/haskell-numerical.html#tensor-product-reduction-and-accumulation",
    "title": "Haskell for numerical computation?",
    "section": "Tensor product: reduction and accumulation",
    "text": "Tensor product: reduction and accumulation\nHere is a common example in quantum information/computation, where we have some quantum states that are complex matrices, and we want to perform a tensor product on them so that we can operate on them collectively.\n\nGiven a list of matrices, the function tensor_all returns the tensor product of all the elements of the list in sequence, e.g., given the list of matrices Ms = [A, B, C], tensor_all(Ms) should return their tensor product A\\otimes B\\otimes C.\n\n\nPythonHaskell\n\n\nimport numpy as np\n\ndef tensor_all(Ms):\n    tensor_prod = 1\n    for M in Ms:\n        tensor_prod = np.kron(tensor_prod, M)\n\n    return tensor_prod\n\n\nimport Numeric.LinearAlgebra\n\ntensor_all :: [Matrix C] -&gt; Matrix C\ntensor_all = foldl kronecker 1\n\n\n\nImperatively in the Python example, we loop through each element of the list and apply the tensor product (kronecker product np.kron) element-by-element with an accumulator, i.e., in the first loop we have (1) \\otimes A, in the second loop we have (1\\otimes A) \\otimes B, in the third loop we have (1\\otimes A \\otimes B) \\otimes C, and so on and so forth, where our accumulator tensor_prod is the value in the parenthesis, is initialized as 1, and is updated every loop.\nNote we have used the hmatrix library which is imported by import Numeric.LinearAlgebra. This documentation is a good resource for a comparison between hmatrix and Python’s numpy.\nIn Haskell, it should be obvious that we can perform a recursion like the case for sum. However, here we introduced the function foldl, which stands for “fold left”. foldl takes in a binary function (in this case kronecker), the initial accumulator (in this case 1), and a list (e.g., [a, b, c]). It then returns kronecker(kronecker(kronecker(1, a), b), c). There is also foldr, or “fold right”, which instead returns kronecker(a, kronecker(b, kronecker(c, 1))).\n\n\n\n\n\n\nPoint-free style programming\n\n\n\n\n\nNote that in the Haskell example, we could have written the function as tensor_all ms = foldl kronecker 1 ms, where the input to the function, the list of matrices ms, is specified. However, since ms is simply the input to the corresponding functions on both sides of the expression, we can omit it and simply write tensor_all = foldl kronecker 1. This is referred to as point-free style programming.\n\n\n\nIf instead, we want to “accumulate” the result of each loop into a list, i.e., we want the function to return [A, A \\otimes B, A\\otimes B\\otimes C], then, in the Python example we have to append the accumulator tensor_prod in each loop into a list. On the other hand, in the Haskell example, we can simply replace foldl with scanl.\n\nPythonHaskell\n\n\nimport numpy as np\n\ndef tensor_all(Ms):\n    tensor_prod = 1\n    tensor_prod_list = []\n    for M in Ms:\n        tensor_prod = np.kron(tensor_prod, M)\n        tensor_prod_list.append(tensor_prod)\n\n    return tensor_prod_list\n\n\nimport Numeric.LinearAlgebra\n\ntensor_all :: [Matrix C] -&gt; [Matrix C]\ntensor_all = scanl kronecker 1\n\n\n\nFolds and scans are part and parcel of a functional programmer, and are common in functional programming languages, as well as array programming languages. Typically, they are referred to as reduction and accumulation, e.g., in the array programming languages NumPy, Julia, and R, folds and scans are called reduce and accumulate.\n\n\n\n\n\nImage credit: code_report\n\n\nIf you would like to know more about array languages, and how they compare to functional languages like Haskell, I highly recommend the youtube channel code_report."
  },
  {
    "objectID": "posts/haskell-numerical.html#collision-model-currying-and-laziness",
    "href": "posts/haskell-numerical.html#collision-model-currying-and-laziness",
    "title": "Haskell for numerical computation?",
    "section": "Collision model: currying and laziness",
    "text": "Collision model: currying and laziness\nLet’s consider a less simple example to reveal more of Haskell’s features:\n\nA superconducting qubit \\rho (rho) is sensitive to heat, and is continuously undergoing a thermalization process. We can model this thermalization process with a collision model, where the qubit evolves via repeated “collision” with different qubits at each discrete time step for n (n) number of time steps. The collision is implemented by the function collision. We want to determine how the von Neumann entropy of \\rho, S(\\rho), changes in each collision up till n time steps. The calculation of the von Neumann entropy is implemented by the function entropy.\n\nFor completeness, the function collision performs the operation of \n\\rho_{t+1}\n= \\sum_i K_i \\rho_t K_i^\\dagger,\n where K_i are Kraus operators, and the function entropy computes \nS\\left(\\rho_t\\right) = -\\mathrm{Tr} \\left(\\rho_t \\log \\rho_t\\right).\n Therefore, what we want is simply the list of \\small[S(\\rho_0), S(\\rho_1), \\ldots, S(\\rho_{n-1})]\nThis might look complicated if one does not know quantum mechanics. However, if we assume that the collision and entropy calculations are done for us in the functions collision and entropy, then we simply want a list of n entropy values, i.e., entropy_list = [entropy(rho), entropy(collision(rho)), entropy(collision(collision(rho))),...].\n\nPythonHaskell\n\n\nimport scipy as sp\n\ndef collision(ks, rho):\n    return sum([k @ rho @ k.conj().T for k in ks])\n\ndef entropy(rho):\n    return - np.trace(rho @ sp.linalg.logm(rho))\n\ndef calc_entropies(ks, rho, n):\n    entropy_list = []\n    for _ in range(n):\n        rho_entropy = entropy(rho)\n        entropy_list.append(rho_entropy)\n        rho = collision(ks, rho)\n\n    return entropy_list\n\n\ncollision :: [Matrix C] -&gt; Matrix C -&gt; Matrix C\ncollision ks rho = sum [k &lt;&gt; rho &lt;&gt; tr k | k &lt;- ks]\n\nentropy :: Matrix C -&gt; Float\nentropy rho = - trace (rho &lt;&gt; logm rho)\n  where\n    logm = matFunc log\n\ncalc_entropies :: [Matrix C] -&gt; Matrix C -&gt; Int -&gt; [Float]\ncalc_entropies ks rho n = map entropy rho_list\n  where\n    rho_list = take n (iterate (collision ks) rho)\n\n\n\n\n\n\n\n\n\nExtra things to note for the Haskell example\n\n\n\n\n\n\nSimilar to Python, Haskell has list comprehension, which has a mathematical syntax as seen in the function collision, e.g.,\nexample = [p * q | p &lt;- ps, q &lt;- qs, p &gt;= 5, q /= 0]\nwould give a list of the set \\{pq\\ |\\ p \\in P,\\ q \\in Q,\\ p \\geq 5,\\ q \\neq 0 \\}.\nOn the other hand, Python’s list comprehension is less concise:\nexample = [p * q for p in ps for q in qs if p &gt;= 5 and q != 0]\nIn the entropy and calc_entropies functions, we have used the where keyword, which allows us to break a function into smaller constituents, e.g.,\nexampleFunc a b c = n / d\n  where\n    n = x + y\n    d = x - y\n    x = a + b + c\n    y = a * b * c\nAlternatively, we can also use the let and in keyword:\nexampleFunc a b c =\n  let\n    n = x + y\n    d = x - y\n    x = a + b + c\n    y = a * b * c\n  in\n    n / d\n\n\n\n\nThe Python example, being imperative, should be self-explanatory. Instead, we are interested in the Haskell example, specifically the calc_entropies function, which might look arcane if one does not know common functional programming functions such as map, take, and iterate. Let’s look at this line-by-line.\n\nCurrying\nThe very first line of the calc_entropies function is the type signature of:\n\n\\mathtt{calc\\_entropies :: [Matrix\\ C]\\ -&gt; Matrix\\ C\\ -&gt; Int\\ -&gt; [Float]}\n\nwhich states that the function calc_entropies takes in a list of complex matrix, a complex matrix, and an integer, and returns a list of floats. The list of complex matrix refers to the Kraus operators K_i used in the collision function, the complex matrix refers to rho or \\rho, the integer refers to n, while the list of floats refers to the output entropy_list.\nOne might notice that [Matrix C] -&gt; Matrix C -&gt; Int -&gt; [Float] doesn’t seem to make a clear distinction between inputs and outputs. This has to do with the concept of partial function application, or currying (also named after Haskell Curry).\nImplicitly, what’s happening is the following:\n\n\\mathtt{calc\\_entropies :: [Matrix\\ C]\\ -&gt;\\ } \\underbrace{\\mathtt{Matrix\\ C\\ -&gt; Int\\ -&gt; [Float]}}_\\text{function $f$}\n \nf\\ \\mathtt{:: Matrix\\ C\\ -&gt;\\ } \\underbrace{\\mathtt{Int\\ -&gt; [Float]}}_\\text{function $g$}\n \ng\\ \\mathtt{:: Int\\ -&gt; [Float]}\n\nwhere the function calc_entropies takes in [Matrix C] as its input, and returns a function f as its output. The function f then takes in Matrix C as the input and returns another function g as the output. Finally, the function g takes in Int as the input and returns [Float] as the final output. This means that functions in Haskell are indeed pure in that they only take in one input and one output, and any functions that appear to take in multiple inputs are in fact taking in only one input and returning a function that takes in also one input, and so on and so forth. This is called currying. Because of this, we can also apply the functions “partially”, for example:\nf a b = a + b\ng = f 5\nwhere we only provide one argument to f when it is expecting two, to create a new function g. This means that f 5 10 and g 10 would give the same output of 15. Currying or partial function application can be a powerful tool for the abstraction and expressiveness of your code.\n\n\n\n\n\n\nExample of the usefulness of currying\n\n\n\n\n\nUnitary operations are common in quantum mechanics, and are how quantum computers perform computations on quantum states or qubits. They are defined as follows: \n|\\psi'\\rangle = U |\\psi\\rangle.\n\nWe can define a function unitary_oper to implement this.\n\nPythonHaskell\n\n\ndef unitary_oper(U, ket):\n    return U @ ket\n\n\nunitary_oper :: Matrix C -&gt; Matrix C -&gt; Matrix C\nunitary_oper u ket = u &lt;&gt; ket\n\n\n\nThere are many common unitaries U used in quantum computations, referred to as quantum logic gates. For example there are the Pauli X gate and the Hadamard gate which are defined as \nX =\n\\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix},\\quad\nH = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{pmatrix}.\n\nCurrying or partial-function application allows us to easily define these new gate operations on top of the existing unitary_oper function:\npauli_x = unitary_oper (2&gt;&lt;2) [0, 1, 0, 1]\nhadamard = unitary_oper (cmap (/ sqrt 2) (2&gt;&lt;2) [1, 1, 1, -1])\nWe can then easily apply the Pauli X and Hadamard gates on a qubit |0\\rangle = [1, 0]^T with pauli_x (2&gt;&lt;1) [1, 0] and hadamard (2&gt;&lt;1) [1, 0]. This can make the code more concise and clearer.\n\n\n\n\n\nLazy evaluation\nMoving on to the function itself:\ncalc_entropies ks rho n = map entropy rho_list\n  where\n    rho_list = take n (iterate (collision ks) rho)\nThe first line features the function map, which is another part and parcel of the functional programmer. It simply apply a function to every element of a list:\n\n\\underbrace{\\mathtt{map\\quad entropy}}_\\text{apply entropy function to each element of}\\quad \\mathtt{rho\\_list}\n\nand has the corresponding Python code of\nentropy_list = [entropy(rho) for rho in rho_list]\n\n# or alternatively\nentropy_list = []\nfor rho in rho_list:\n    entropy_list.append(entropy(rho))\nFinally, in the last line we have\n\\mathtt{where\\quad rho\\_list} = \n\\underbrace{\\mathtt{take\\quad n}}_\\text{take first $n$ of}\\quad \\underbrace{\\mathtt{(iterate\\quad (collision\\quad ks)\\quad rho)}}_\\text{returns an infinite-sized list of [rho, collision(rho), collision(collision(rho)),...]}\n which gives rho_list = [rho, collision(rho), collision(collison(rho)),...] up till n number of elements in the list.\nAlso note that we can reduce the number of parenthesis by using $ instead, e.g., rho_list = take n $ iterate (collision ks) rho.\nIt might be quite surprising that the iterate function returns an infinite-sized list. For example, iterate add1 10 gives [11, 12, 13, 14, 15, ...] to infinity, where the function add1 is applied on 10 ad infinitum. If we were to write the corresponding Python code for rho_list = take n (iterate (collision ks) rho), it might look something like\nrhos = []\nwhile True:\n    rhos.append(rho)\n    rho = collision(ks, rho)\n\nrho_list = rhos[:n]\nwhich runs forever and result in a list of infinite size. However, this is not a problem in Haskell, thanks to the fact that Haskell is lazy-evaluated. This means that Haskell only evaluate values when required, and so since we only require the first n elements of the infinite list, Haskell only evaluate that first n elements.\nIf this Python code is lazy-evaluated like in Haskell, one might imagine that the program looks ahead and saw that it only require the first n elements as stated in rho_list = rhos[:n], and so it stops the while True loop after only running n times."
  },
  {
    "objectID": "posts/quantum-computing.html",
    "href": "posts/quantum-computing.html",
    "title": "An honest explanation of quantum computing",
    "section": "",
    "text": "We have all heard it before:\n\n“It’s doing all possible computations simultaneously!”\n“A quantum bit can be both 0 and 1 at the same time!”\n“Quantum computers compute in parallel universes!”1\n1 I’m shocked that this is no longer an exaggeration.\nI get it, it’s difficult to talk about quantum mechanics without resorting to technical jargon, forcing us to rely on analogies that barely make sense.\nBut I mean, if it’s doing multiple computations simultaneously (in parallel universes or not), how many computations (or parallel universes) is it exactly?\nAnd we can already do computations in “parallel universes” with conventional computers. Its called multithreading and multiprocessing.\nSo does that mean that I can achieve the same advantage in principle if I simply match the number of conventional computers to the number of parallel universes?\nYou see, these popular explanations for how a quantum computer works are typically a gross simplification that fails when probed a little. The fact that they even exist in the first place surprises me because the concepts behind quantum computing is actually quite simple.\nYes, that’s right. I believe that the basics of quantum computing is simple enough that any reasonable layperson can understand it.\nThe mystique only arises when we conflate the metaphysical interpretations of quantum mechanics2 with the explanations of the quantum computations that they can perform. This is akin to explaining classical computation with some metaphysical interpretation of the nature of electricity, e.g., “The reason your computer can calculate 1+1 is because electricity running in your computer can flow on top of each other and thus adds up.” which is not what happens at all.\n2 To be fair, these interpretations of quantum mechanics are indeed weird and mind-blowing. But again, no matter how weird electricity is, classical computer science is independent of it. Quantum computing should be viewed the same way. Also keep in mind that these are just interpretations with no evidence of being true.So what exactly is this simple concept behind quantum computing?\nIt’s a sphere.\n\nEnters the Bloch sphere\n\n\n\nThe Bloch sphere with an arrow inside pointing outwards from the center.\n\n\nThis is called a Bloch sphere, and it captures everything about a qubit (even all the alleged parallel universes).\nAnyone can understand and visualize a sphere, so it puzzles me as to why we do not start with this instead of parallel universes or “being 0 and 1 at the same time”. The Bloch sphere is also mathematically accurate. That is, it’s not an analogy, it’s the truth!\nIn fact, the Bloch sphere also captures classical computing. The familiar logical 0 and 1 bits from classical computing are at the North and the South poles, respectively. In a conventional computer, imagine a bunch of these spheres with an arrow in each pointing only to either the North pole (bit 0) or the South pole (bit 1).\n\n\n\n8 Bloch spheres representing a sequence of 8 classical bits.\n\n\nDuring a classical computation, the arrows simply flip between the North and the South poles, most of the time conditioned on other arrows of other spheres3. The sequence of arrows then represents the results of the computation.\n3 E.g., a NOT operation flips 0 (North) to 1 (South) and 1 (South) to 0 (North)What about a qubit?\nWell, we simply free the arrow such that it is able to point to anywhere on the sphere. That’s it. Where the arrow points is the state of the qubit.\nSo when people say a qubit can be “0 and 1 at the same time”, what they actually mean is that the arrow is pointing somewhere between the North (0) and the South (1) poles. Typically this is at the equator, halfway between the poles.\nOf course, there is not just one location on the equator of a sphere. Singapore is at the equator, so is Ecuador, and so are multiple other countries on Earth. This means that two qubits that are “0 and 1 at the same time” can be different. To differentiate them (figure out where their arrows point), or to pinpoint a location on Earth, we require two coordinates, the latitude, and the longitude.\nSince the Bloch sphere is also a sphere, the “latitude” and the “longitude” are two variables (or degrees of freedom) that also defines a qubit: \\theta the “latitude”, and \\varphi the “longitude” also known as the relative phase.\n\n\n\n8 Bloch spheres representing a sequence of 8 qubits.\n\n\nSo what happens during a quantum computation?\nWell, like the classical case, the arrow in each qubit simply jumps (or rotates) around to and from any point on the sphere4.\n4 In general, this is done by performing unitary operations (literally rotations) on it. Typically, these operations can be described by simpler quantum logic gates like the logic gates in the classical case.So all these talks of a quantum computer being able to perform computations faster because of “superposition” or because it can performs computation “simultaneously” etc., simply refers to the fact that there is an increase in the degrees of freedom for the computation as compared to classical digital computations.\nIn other words, this freedom for the arrow (its state) to rotate and point to anywhere on the sphere, allows certain computing tasks to require less operations (flipping or rotating of arrows), than if the arrows are confined to the North and South poles.\n\n“But what’s with all the weirdness of quantum physics then?”\n\nIt’s true that in actual quantum systems that implements a qubit, there are typically two preferred states (the North and South poles of our Bloch sphere). These are sometimes called pointer states. We observes these preferred states in our classical world as they are what remains after the quantum system loses its coherence. But in reality, the quantum system sees no difference between these preferred states and any other states it could take (e.g., at the equator of our Bloch sphere).\nSince we only observes these preferred states (and that our macroscopic reality is built out of them), we deemed them as “natural” which led to these apparent weirdness, e.g., how can something that takes the “natural” states of spin up and spin down, be placed in the “unintuitive” state of “spin up and spin down at the same time”?\nSo there are some credence to the weirdness and hype. However, again, just like how the computer science of classical computation is independent of the physics of electricity and semiconductors (or their interpretations) etc., the computer science of quantum computation should also be seen as separate from the interpretations of what’s happening in the physical systems that implements them.\n\n\n\n\n\n\nExtra notes\n\n\n\n\n\nThe attentive reader will notice that a sphere is spherically symmetric. Just like on the Earth, there are no real ups and downs except for those defined by convention.\nWhat this means is that we can orient the Bloch sphere however we want. Pick any two points at opposite ends of the sphere and we can define that as the 0 and 1. The reason the North and the South poles are defined as the 0 and 1 is simply convention.\nNow that you know this, you should realize that the idea that a qubit being “both 0 and 1 at the same time” somehow grants an advantage is quite silly, since it’s just another point on the sphere, no different from the North (0) and the South (1) poles.\nA more attentive reader will also ask “what about the insides of the sphere?”.\nThe reader is right. There is actually a third degree of freedom: the “radius”, which describes the length of the arrow. So yes, the arrow can point to somewhere inside the sphere. These are called mixed states, whereas arrows that point to the surface of the sphere are called pure states.\nDuring a quantum computation, the qubit is sensitive to thermal noises that rotate the arrow in a manner that is undesired. These noises also shortens the arrow’s length to some fixed point inside the sphere depending on the temperature, in a process known as decoherence. This is why (depending on the implementation) quantum computers need to be kept at a very cold temperature, so that they can perform computations before they eventually reach the fixed point inside the sphere.\n\n\n\n\n\nEntanglement — the actual spice\nThe Bloch sphere might reminds one of analog computers, where quantities can take a continuum of values instead of the 0s and 1s in digital computers. In fact, if we can build an analog computer with two degrees of freedom like the Bloch sphere, then it can function exactly like a single qubit.\nThe “quantumness” only comes in if we include the concept of entanglement5. Entanglement is the main spice that makes quantum systems special, preventing it from being replicated by classical systems, e.g., the analog computer with two degrees of freedom.\n5 Entanglement is often conflated with the phrase “spooky action at a distance”. The latter describes the apparent nonlocal collapse of the wave function. Einstein, Podolsky, and Rosen (1935) used entanglement as an example to illustrate the significance of this nonlocal collapse, which eventually led to this conflation.Imagine what would happen if we have two qubits.\n\n\n\nTwo Bloch spheres.\n\n\nOne might think that there are now six variables to describe the pair of qubits (where both arrows point) since we now have two Bloch spheres6. However, this is only true if there is no correlation between the pair of qubits, that is, the two arrows are independent of each other.\n6 If you thought the number is four instead, take a look at the Extra notes above.Typically, when the two qubits interact with each other (or undergo quantum computations together), they can become entangled, which means that their two arrows (their states) are now correlated in a way that is impossible to achieve classically.\nThis is akin to them becoming a single (hyper)sphere with a single arrow pointing at some point that represents their combined state. The dimension of this (hyper)sphere will be the number of variables or degrees of freedom required to describe them.\nQuite surprisingly, there can be a total of up to 15 degrees of freedom!\n\n\n\nIt’s impossible to draw a 15 dimensional sphere, so here’s a fun visual instead.\n\n\nSix variables describe the two qubits independently (three for each), while the remaining nine describes how they are correlated to each other.\nIt is important to note that this single 15 dimensional Bloch (hyper)sphere describes both qubits together, and it cannot be broken down into separate descriptions for each qubit (without losing information about the entanglement). This means that the two entangled qubits can only be described as single object.\nThis is the stark feature of quantum systems that sets it apart from classical systems. Supposed that we were to build two analog computers with three degrees of freedom each to simulate the two Bloch spheres, before attempting to correlate their states (perhaps by performing some joint operations on them). We can never correlate them to the extent that entanglement could7.\n7 For classical correlations (no entanglement), they can be represented as a weighted sum of multiple pairs of independent Bloch spheres. These are called separable states.In addition to the increased degrees of freedom, this increased correlation gained by entanglement can also be exploited in various quantum computational tasks.\n\n\nIs that it?\nYep, that’s it.\nQuantum computation is simply the rotations of these arrows on/in Bloch spheres8. While classical computing is confined to flipping between the North and South poles, quantum computing allows arrows to freely rotate anywhere on the Bloch sphere.\n8 With some caveats like the fact that we can’t determine where the arrows point (read-out of the state) directly, which we did not touch upon.Where the arrows point and how they rotate is how we store and process information, just like how a sequence of North and South poles (0s and 1s) and how they flip, encodes, and processes information in the classical case.\nBy combining multiple qubits, the allowable space or degrees of freedom for this information processing increases. Combined with entanglement, this information processing can take place in even higher dimensional space, with some algorithms exploiting the increased correlation.\nThe main “essence” of quantum computing is this change in computing paradigm, regardless of whether quantum systems that implement it do so with the help of parallel universes or not.\nAgain, I want to emphasize the separation between the implementation’s physics (and its interpretation), and this new computing paradigm that grants these advantages over classical computing.\nThe difficulty of course, is coming up with algorithms that not only can utilize the increased degrees of freedom, but also can be better or faster than the classical case. It turns out however, that classical computing (being confined to the North and South poles) is good enough in most cases, and quantum algorithms that can be better are rare.\nThis might be because in classical digital computing, how we encode and process information are quite straightforward, and decades of creating higher abstractions means that no programmers are flipping individual bits. However, in the relatively young field of quantum computation, physicists are still concerned with the individual operations or rotations to perform on individual or groups of qubits.\nPerhaps one day quantum computing will reach the same status as classical computing. But for now, we are stuck with rotating arrows.\n\n\n\n\n\n\n Back to topReferences\n\nEinstein, A., B. Podolsky, and N. Rosen. 1935. “Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?” Physical Review 47 (10): 777–80. https://doi.org/10.1103/physrev.47.777."
  },
  {
    "objectID": "posts/why-fedora.html",
    "href": "posts/why-fedora.html",
    "title": "Why I recommend Fedora for new Linux users",
    "section": "",
    "text": "Here, I provide a list of reasons on why I think Fedora is a good starting point for new Linux users, which comes down to a single advantage: good defaults.\n\nSemi-rolling release\nWhen you use your computer, here’s what you might expect:\n\nYou want all your programs to be updated to the latest versions.\nYou want the stuffs running in the background to be updated to the latest versions.\nYou don’t want major changes to your user experience or to how things are done.\n\nThis is reasonable; you want frequent updates because you want the latest features, or security and bug fixes. But at the same time, you don’t want your desktop to completely change after an update (think Windows 7 to 8, or 10 to 11).\nFedora’s semi-rolling release cycle abide by these: you get frequent updates (including kernel updates), but some packages like the desktop environment GNOME, or major software decisions (like moving to a new technology), are only updated in a major version release that the user must initiate.\n\n\nWayland\nMost Linux distros use X11 as its windowing system (a program that handles how windows are drawn on your screen). However, X11 does not have GUI isolation, allowing different programs to interact with one another. A malicious program can then record your screen, or log the keys you typed in your browser while you are logging in to your bank.\nWayland is the successor to X11, and is developed with security in mind, isolating each window from one another.\n\nUnlike X, the Wayland input stack doesn’t allow applications to snoop on the input of other programs (preserving confidentiality), to generate input events that appear to come from the user (preserving input integrity), or to capture all the input events to the exclusion of the user’s application (preserving availability).\n— Jake Edge, LWN.net\n\n\n\nSwap on ZRAM\nLinux systems offers a “swap” space, where memory on the RAM can be copied to disk space, freeing the RAM in the process. Swap is an important part of a Linux operating system. However, sensitive information stored on RAM can be copied to disk, risking potential security issues. ZRAM solves these problems by having the swap space remain in the RAM with compression, ensuring that no sensitive information will be stored on disk.\n\n\nBtrfs\nMost Linux distros have traditionally used the ext4 filesystem. btrfs is a newer file system that offers advantages over ext4:\n\nCompression leads to less disk space used.\nCompression also extends the lifespan of flash-based devices (SSDs, etc.).\nLess likely to have data corruption.\nProvides snapshots that allows rollback when system breaks.\n\n\nFor laptop and workstation installs of Fedora, we want to provide file system features to users in a transparent fashion. We want to add new features, while reducing the amount of expertise needed to deal with situations like running out of disk space. Btrfs is well adapted to this role by design philosophy, let’s make it the default.\n…\nThe Btrfs community has users that have been using it for most of the past decade at scale. It’s been the default on openSUSE (and SUSE Linux Enterprise) since 2014, and Facebook has been using it for all their OS and data volumes, in their data centers, for almost as long. Btrfs is a mature, well-understood, and battle-tested file system, used on both desktop/container and server/cloud use-cases.\n— Fedora Project wiki, Btrfs changes proposal\n\n\n\nSELinux and firewalld\nSecurity Enhanced Linux (SELinux) provides Mandatory Access Control (MAC).\n\nIn computer security, mandatory access control (MAC) refers to a type of access control by which the operating system or database constrains the ability of a subject or initiator to access or generally perform some sort of operation on an object or target.\n— Wikipedia\n\n\nSecurity Enhanced Linux (SELinux) provides an additional layer of system security. SELinux fundamentally answers the question: May &lt;subject&gt; do &lt;action&gt; to &lt;object&gt;?, for example: May a web server access files in users’ home directories?\n— Fedora docs\n\nFedora comes with good default policies for SELinux, as well as good default firewall rules with firewalld.\n\n\nFlatpak enabled\nFlatpak is supposed to be a new and universal way of installing programs. Flatpak is now available on almost all major distros and is also the most popular way of installing programs on the Steam Deck.\nUbuntu on the other hand, while often recommended to beginners, decided to drop Flatpak in favor of snap, which is their own alternative that is much less popular. snap packages masquerade as traditional packages, which can cause confusion for new users. They also encourage users to sign up an Ubuntu One account just to install snap packages without root.\n\n\nSpeed of adopting new technologies\nFedora tends to adopt new technologies at a fast pace, such as Wayland, swap on ZRAM, Flatpak, PipeWire, btrfs etc. New technologies tends to offer improvements in terms of usability and security.\nFor example, Fedora adopted Wayland in 2016, but Ubuntu only done so in 2021. Even Red Hat Enterprise Linux (RHEL) and Debian adopted Wayland in 2019, earlier than Ubuntu, despite both being extremely conservative distros.\n\n\nWhat’s next?\nIf you just want a usable system with no interest in learning more about the Linux ecosystem, then I believe Fedora is the ideal choice. However, I do not encourage people to stay on Fedora for the rest of their Linux journey. Rather, one should try out different Linux distros and explore alternatives to learn more about the Linux ecosystem.\nEventually, as you gain more understanding of the Linux ecosystem, you will develop your own opinions on things. For example, you might be against Flatpak, or perhaps you might think that systemd has major problems. Regardless, the greatest strength of the Linux ecosystem is the power to make your own choices on what software and technologies you prefer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/betty-hill-star-map.html",
    "href": "posts/betty-hill-star-map.html",
    "title": "The Betty Hill Star Map: Where are my aliens from?",
    "section": "",
    "text": "Disclaimer\n\n\n\nI apologize for the lack of sources provided in this post. It is quite difficult to find accessible primary sources, especially for these kinds of topics where most details are only accounted in books that are not freely available. If you do not see a source, take it with a grain of salt.\nIn Ufology “lore”, it is widely accepted/parroted that the well-known, short, enlarged head, with large black eyes, Grey alien comes from the Zeta Reticuli binary star system.\nThis is due to a myriad of (supposedly) independent “abductees”, “contactees”, or “insiders”, who have made mentions of Zeta Reticuli (as well as its association with the Greys) in their “lore”, e.g., Bob Lazar, Project Serpo, David Icke, Ashtar, Cosmic Awareness, or the various “Galactic Federation” “channelings”1.\nThe very first association (as far as I know) of these Greys with the Zeta Reticuli star system originates from the Barney and Betty Hill incident back in the 1960s. Particularly, from this incident came a “star map”, crudely drawn from memory under hypnotic regression 2 years after the alleged incident, which vaguely resembles a constellation of stars that include the Zeta Reticuli binary star system.\nToday, let us examine this star map with modern tools and data, and determine if there is any credence to this association with the Zeta Reticuli star system.\nBut first, let us have some context."
  },
  {
    "objectID": "posts/betty-hill-star-map.html#fishs-interpretation",
    "href": "posts/betty-hill-star-map.html#fishs-interpretation",
    "title": "The Betty Hill Star Map: Where are my aliens from?",
    "section": "Fish’s interpretation",
    "text": "Fish’s interpretation\nFor the convenience of the reader (click to enlarge). \n\n\n\nMarjorie Fish’s intepretation of the star map using data from the Gliese catalog.\n\n\nAdmittedly, it looks quite similar to Betty Hill’s star map, and even more impressive is that this interpretation was derived from physical models of nearby stars (assuming our Sun is in the star map) that Fish herself built using data from the Gliese catalog.\nDue to the large numbers of stars, Fish filtered out stars based on exobiological considerations. It took years but she succeeded in finding a constellation of stars and vantage point that resembles Betty Hill’s star map using her homemade model.\nThere are a lot of differences however, e.g., the line to Alpha Mensae is much longer than in Betty Hill’s star map, while the line to Gliese 86 is much shorter. The distance between Tau Ceti and 82 Eridani is also much shorter than expected. All in all, it is not a perfect match.\nDespite that, Fish’s interpretation became so popular that it was even discussed by Carl Sagan on Cosmos5. Sagan’s argument is that if one can pick and choose from a large number of stars from any vantage point, then one can always find a constellation of stars that resembles Betty Hill’s star map6.\n5 Sagan demonstrated that by removing the lines, there are hardly any resemblance between the two maps at all. 6 More on this later as we attempt to try this out for ourselves.Nevertheless, the most significant result of Fish’s interpretation is that the two big circles (often thought of as the home systems of the aliens) turns out to be the Zeta Reticuli binary star system. This is what sparked the beginning of the association between Grey-type aliens and Zeta Reticuli.\n\n\n\n\n\n\nMisleading visuals of Fish’s interpretation\n\n\n\n\n\nThe reader should take note that there are various misleading visualizations of the Fish’s interpretation that are often being shared online. These misleading visuals exaggerate the similarity between Fish’s interpretation and Betty Hill’s star map. Some examples are the one on Wikipedia:\n\nand this one that is often shared on social medias that is just blatantly misleading:"
  },
  {
    "objectID": "posts/betty-hill-star-map.html#reproducing-fishs-interpretation",
    "href": "posts/betty-hill-star-map.html#reproducing-fishs-interpretation",
    "title": "The Betty Hill Star Map: Where are my aliens from?",
    "section": "Reproducing Fish’s Interpretation",
    "text": "Reproducing Fish’s Interpretation\nLet us attempt to reproduce Fish’s results by using the Hipparcos catalog from the 1990s7, which contains much more precise data than the Gliese catalog that Fish used in 1969. Specifically, we will create a 3D model of nearby stars within 30 parsecs of our Solar System.\n7 The Gaia catalog was considered, but it excludes a number of bright stars and the star designations are too long and cumbersome to work with.The lines of codes are quite short, and I encourage the interested reader who wants their own 3D model to follow along, especially if one has experience in Python.\n\nGetting the positions of nearby stars\nWe can easily retrieve the catalog as a pandas dataframe in Python with the Skyfield package.\n\n\n\n\n\n\nSkip this if you are not interested.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom skyfield.api import Star, load\nfrom skyfield.data import hipparcos\n\nwith load.open(hipparcos.URL) as f:\n    df = hipparcos.load_dataframe(f)\nTo get the positions of nearby stars relative to our Sun, we first retrieve the JPL ephemeris DE421, which contains the positions of major bodies in the Solar System from 1900–2050, before finding the Sun’s position relative to the Solar System’s barycenter on 19 September 1961.\nThese are actually mostly unneeded information, but Skyfield requires a Barycentric object to retrieve the observed position of stars with respect to that object’s position at a certain time, and we will have to make do since we want Skyfield to do the calculations for us.\neph = load(\"de421.bsp\")\nsun = eph[\"sun\"]\n\nts = load.timescale()\nt = ts.utc(1961, 9, 20)\nsun_barycentric = sun.at(t)\nNext, we let Skyfield calculate the observed position (in terms of x, y, z coordinates and distance relative to the Sun) of each star in the dataframe of the catalog, before storing them in their corresponding list. We also convert the units from AU (astronomical unit) to parsec as it’s easier to work with. Note that hips contains each star’s designation in the catalog, e.g., the star Sirius has the designation HIP 32349.\nau_1_parsec = 206265\n\ndistances_pc = []\ncoords_pc_x = []\ncoords_pc_y = []\ncoords_pc_z = []\nhips = []\nfor i, row in df.iterrows():\n    star = Star.from_dataframe(row)\n    star_astrometric = sun_barycentric.observe(star)\n    _, _, distance_au = star_astrometric.radec()\n    coords_au = star_astrometric.xyz\n\n    distances_pc.append(distance_au.au / au_1_parsec)\n    coords_pc = coords_au.au / au_1_parsec\n    coords_pc_x.append(coords_pc[0])\n    coords_pc_y.append(coords_pc[1])\n    coords_pc_z.append(coords_pc[2])\n    hips.append(i)\nFinally, we store these data for each star in a new dataframe, filtered it such that we only keep stars within 30 parsecs (~100 light years), leaving us with just 2350 stars, before saving it to a CSV file.\ndf_new = pd.DataFrame(\n    np.array([hips, coords_pc_x, coords_pc_y, coords_pc_z, distances_pc]).T,\n    columns=[\"hip\", \"coords_x\", \"coords_y\", \"coords_z\", \"distances\"],\n)\n\ndf_near = df_new[df_new[\"distances\"] &lt; 30]\ndf_near.to_csv(\"30pc_stars.csv\", index=False)\n\n\n\nNote that the obtained positions are the observed positions from our Solar System, that is, the stars actual positions would have changed due to the time it takes for light from these stars to reach us, and an alien star map would probably use the latter instead of the observed positions, let alone our observed positions. However, the difference are negligible and this save us some calculations, so we will stick with this.\n\n\nPlotting the nearby stars\nNext, we will plot these 2350 stars (+ the Sun) in Blender. Blender is chosen because it’s fun, and it’s easy to control and animate the camera when we are looking for different vantage points. The hybrid graphical and Python workflow also greatly makes our lives easier as Python can programmatically handle the placing of the 2350 stars at the right position, while I can easily change the looks of the stars in Blender’s GUI8.\n8 One can also use programs like Gaia Sky or Space Engine to do their own investigation.\n\n\n\n\n\nSkip this if you are not interested\n\n\n\n\n\nFirst, let us load the CSV file that we saved earlier and store them in the list stars.\nIt’s quite troublesome to install external packages to Blender’s builtin Python, and so we used the builtin csv package instead of pandas dataframe this time.\nimport bpy\nimport csv\n\nwith open(\"30pc_stars.csv\") as f:\n    reader = csv.DictReader(f, delimiter=\",\")\n\n    stars = []\n    for entry in reader:\n        stars.append(entry)\nIn Blender, I have prepared different materials for the Sun (sun), stars in Fish’s interpretations (starmap), and the rest of the stars (starlight). We can refer to these materials in Python as follows.\nsun_mat = bpy.data.materials.get(\"sun\")\nstarmap_mat = bpy.data.materials.get(\"starmap\")\nstarlight_mat = bpy.data.materials.get(\"starlight\")\nI also made a helper function to easily place stars as UV spheres.\ndef uv_sphere_add(radius=1, name=None, location=(0, 0, 0)):\n    bpy.ops.mesh.primitive_uv_sphere_add(radius=radius, location=location)\n    obj = bpy.context.object\n    if name:\n        obj.name = name\n    return obj\nCreating the Sun and adding the sun material to it is as simple as\nsun_obj = uv_sphere_add(0.2, name=\"sun\", location=(0, 0, 0))\nsun_obj.data.materials.append(sun_mat)\nWe can now repeat the process for each star. By checking the HIP designations of the stars in Fish’s interpretation, we can change the materials of these stars to the starmap material, while keeping the starlight material for the rest of the stars. Also note that we scaled the positions up by a factor of 10, i.e., 1 parsec correspond to 10 units of distance in Blender.\nfish_starmap = [\n    \"15330.0\", \"15371.0\", \"29271.0\", \"15510.0\", \"10138.0\", \"8102.0\", \"7981.0\", \"3093.0\", \"7918.0\", \"12843.0\", \"7235.0\", \"11072.0\", \"10798.0\", \"10164.0\",\n]\nfor star in stars:\n    star_obj = uv_sphere_add(\n        0.2,\n        name=star[\"hip\"],\n        location=(\n            float(star[\"coords_x\"]) * 10,\n            float(star[\"coords_y\"]) * 10,\n            float(star[\"coords_z\"]) * 10,\n        ),\n    )\n    if star[\"hip\"] in fish_starmap:\n        star_obj.data.materials.append(starmap_mat)\n    else:\n        star_obj.data.materials.append(starlight_mat)\n\n\n\nThere we have it, we now have a 3D model of 2350 stars within 30 parsecs of our Sun.\n\n\nVideo\nAnimation of stars within 30 parsecs, with stars from Fish’s interpretation colored red, and our Sun colored yellow.\n\n\nAfter connecting the stars accordingly, we can find a vantage point that roughly matches with Fish’s interpretation.\nVideo\nFor the convenience of the reader (click to enlarge). \n\n\n\n3D reproduction of Fish’s interpretation of Betty Hill’s star map.\n\n\nFirst of all, note that Kappa Fornacis is way off. This is due to the more precise data provided by the Hipparcos catalog as compared to the Gliese catalog that Marjorie Fish used. Likewise, Gliese 86.1 is actually nowhere close at almost 200 light years away and thus is missing.\nThese discrepancies were most likely also acknowledged by Marjorie Fish as new data came out9, and might be a deal-breaker as according to Fish’s account of her meeting with Betty Hill, Hill claimed to remember the distinctive triangle formation to the left in addition to the ones connected by lines.\n9 Not that it matters, but according to her niece, Fish did not actually repudiate her star map, and that her obituary saying so was in error.Nevertheless, huge kudos to Marjorie Fish for producing her version of the star map by hand to such a great accuracy."
  },
  {
    "objectID": "posts/betty-hill-star-map.html#randomly-generated-stars",
    "href": "posts/betty-hill-star-map.html#randomly-generated-stars",
    "title": "The Betty Hill Star Map: Where are my aliens from?",
    "section": "Randomly generated stars",
    "text": "Randomly generated stars\nHere, I randomly generated 50 stars with uniformly distributed positions, and attempt to find a vantage point and constellation that resembles Betty Hill’s star map.\nI assure you that there is no cherry-picking and I’m not just drawing lines on a 2D projection of the 50 stars either, but instead ensured that the star map made physical sense in terms of distances10.\n10 Which is why it might look like there are better solutions that I didn’t take, but those solutions only work as a 2D projection from this vantage point, and not in terms of physical distance.\n\n\n3 star maps that somewhat resembles Betty Hill’s star map, created from 3 different sets of randomly generated 50 star positions.\n\n\nAs you can see, it seems that it’s not that difficult to find a constellation of stars that resembles Betty Hill’s star map from just 50 stars, even when these stars are not based on reality. In fact, each one took me just around 5 minutes.\nThese are of course not perfect matches (but neither are Fish’s, Atterberg’s, or Koch-Kyborg’s), and there might be better solutions if given enough time (or using some algorithmic method)."
  },
  {
    "objectID": "posts/betty-hill-star-map.html#a-new-interpretation-of-the-star-map",
    "href": "posts/betty-hill-star-map.html#a-new-interpretation-of-the-star-map",
    "title": "The Betty Hill Star Map: Where are my aliens from?",
    "section": "A new interpretation of the star map",
    "text": "A new interpretation of the star map\nFinally, let me present an alternate interpretation using just star systems in our local neighborhood of up to 22 light years (~7 parsecs) away from us, much closer than Fish’s and Atterberg’s interpretations.\nVideo\nFor the convenience of the reader (click to enlarge). \n\n\n\nMy interpretation of Betty Hill’s star map.\n\n\nIt’s not a perfect match, but again I emphasize that neither are Fish’s, Atterberg’s, or Koch-Kyborg’s.\nIt’s not difficult to find this constellation of stars either:\n\nJust by looking at prominent stars in the vicinity of our Solar System, I realized that the Bernard’s star, Proxima/Alpha Centuri, our Solar System, and Sirius, forms the “V” shape formation that appears on Betty Hill’s star map.\nI then looked for stars in the “foreground” of this “V” shape formation and found Tau Ceti (and YZ Ceti). Quite coincidentally, slightly in front of Tau Ceti (and YZ Ceti) is GJ 1005, a binary star system, exactly what we need to match with Betty Hill’s star map.\n82 G. Eridani seems to be the only possible candidate to the right of GJ 1005, and Van Maanen 2 is right above Tau Ceti (and YZ Ceti). It was then either Tau Ceti or YZ Ceti, Gliese 876 or HIP 114110, HR 7703 or Gliese 784, and Gliese 829 or Altair. I simply chose the ones that best matches with Betty Hill’s star map.\n\nIn my opinion, my interpretation matches better with Betty Hill’s star map than Fish’s in certain parts. I even got the triangle formation on the left (by chance really, it was an afterthought)11. Furthermore, the vantage point is facing towards the galactic center, and is slightly above the galactic plane looking downwards, in other words it is not as arbitrary. A number of these star systems also have (or might have) exoplanets.\n11 If you are concerned about the lone unconnected star at the middle top part of Betty Hill’s star map, Psi Velorum is the only star that appears around that location if we filter out stars with absolute magnitude &gt; 4.The distances and lines drawn also make sense. For example, in Fish’s interpretation, one might wonder why the aliens do not connect 82 G. Eridani or Tau Ceti to Sol/our Sun given that they are right beside each other. In my interpretation, Van Maanen 2 is actually in a different direction to Tau Ceti (it is above Tau Ceti and GJ 1005, you can see it in the video above), and so it’s not surprising that the aliens drew the lines to them separately.\nThis interpretation however, would place the aliens’ origin at GJ 1005 (or Gliese 1005), a binary red dwarf system12. As far as I am aware, there are no candidate exoplanets on GJ 1005 as of the writing of this post, but it is not uncommon for red dwarfs to have some.\n12 Finally somewhere more mundane. I mean why do all the aliens in Ufology lore always have to coincidentally come from some named star systems?I also did not account for how likely it is for an alien species to visit these star systems, e.g., I didn’t check if they are variable stars or close binaries or what not which Fish excluded. However, these stars are all quite close to each other, and there aren’t many stars in the vicinity of these routes for the aliens to “skip” or be “uninterested by” like in Fish’s interpretation.\nIt is important to note that the point here is not to present what I think is the “correct” interpretation of Betty Hill’s star map. Instead, the point is that one can easily find multiple interpretations that resembles it.\nIn other words, it’s pointless to examine the star map to try to figure out where the aliens come from."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Doctor of Philosophy, Physics  Nanyang Technological University (NTU), Singapore\nBachelor of Science, Physics (Honours)  Nanyang Technological University (NTU), Singapore\nDiploma, Electrical Engineering  Ngee Ann Polytechnic, Singapore\n\n2019 - 2023\n 2015 - 2019\n 2010 - 2013"
  },
  {
    "objectID": "cv.html#professional-qualifications",
    "href": "cv.html#professional-qualifications",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Doctor of Philosophy, Physics  Nanyang Technological University (NTU), Singapore\nBachelor of Science, Physics (Honours)  Nanyang Technological University (NTU), Singapore\nDiploma, Electrical Engineering  Ngee Ann Polytechnic, Singapore\n\n2019 - 2023\n 2015 - 2019\n 2010 - 2013"
  },
  {
    "objectID": "cv.html#professional-appointments",
    "href": "cv.html#professional-appointments",
    "title": "Curriculum Vitae",
    "section": "Professional Appointments",
    "text": "Professional Appointments\n\n\nResearch Fellow  Nanyang Technological University (NTU), Singapore\nProject Officer  Nanyang Technological University (NTU), Singapore\nTeaching  Physics Discovery Camp - Computational Nonlinear Lab, NTU  PH3101 Quantum Mechanics 2 (Restricted Repeat), NTU  Physics Discovery Camp - Computational Nonlinear Lab, NTU  PH3101 Quantum Mechanics 2, NTU  PH1199 Physics Lab 1B, NTU  PH1198 Physics Lab 1A, NTU\nIntern  ST Electronics, Singapore\n\n2024 - current\n 2023\n  2025  2025  2024  2021  2020  2019\n 2012"
  },
  {
    "objectID": "cv.html#awards-and-achievements",
    "href": "cv.html#awards-and-achievements",
    "title": "Curriculum Vitae",
    "section": "Awards and Achievements",
    "text": "Awards and Achievements\n\n\nShort-speech Contest Best Presentation  PAP701 Graduate seminar module  Nanyang Technological University (NTU), Singapore\nDean’s List (top 5% of cohort)  Nanyang Technological University (NTU), Singapore\nNTU President Research Scholar (completing URECA)  Nanyang Technological University (NTU), Singapore\nDirector’s List (top 5% of cohort)  Ngee Ann Polytechnic, Singapore\nBest Performance (top student of cohort)  Programmable Logic Device  Digital Electronics & Practice  Ngee Ann Polytechnic, Singapore\n\n2019\n  2017/2018\n 2016/2017\n 2011\n  2011  2010"
  },
  {
    "objectID": "cv.html#bibliography",
    "href": "cv.html#bibliography",
    "title": "Curriculum Vitae",
    "section": "Bibliography",
    "text": "Bibliography\n\nPublications\n\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Non-Markovian refrigeration and heat flow in the quantum switch, Physical Review A, 110(2), 022220 (2024).\nL. Y. Chew, A. Pradana, L. He, and J. W. Cheong, Stochastic thermodynamics of finite-tape information ratchet, European Physical Journal Special Topics (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Effects of non-Markovianity on daemonic ergotropy in the quantum switch, Physical Review A, 108(1), 012201 (2023).\nL. He, J. W. Cheong, A. Pradana, and L. Y. Chew, Effects of correlation in an information ratchet with finite tape, Physical Review E, 107(2), 024130 (2023).\nJ. W. Cheong, A. Pradana, and L. Y. Chew, Communication advantage of quantum compositions of channels from non-Markovianity, Physical Review A, 106(5), 052410 (2022).\nL. He, A. Pradana, J. W. Cheong, and L. Y. Chew, Information processing second law for an information ratchet with finite tape, Physical Review E, 105(5), 054131 (2022).\n\n\n\nConferences\n\nPosters\n\nJ. W. Cheong, A. Pradana, and L. Y. Chew^\\dagger, Enhancement of quantum processes from indefinite causal order through non-Markovianity, 29th International Conference on Statistical Physics (STATPHYS29), Florence, Italy, July 2025.\nJ. W. Cheong^\\dagger, A. Pradana, and L. Y. Chew, Non-Markovian refrigeration and heat flow in the quantum switch, Quantum Thermodynamics Conference 2025 (QTD2025), Singapore, July 2025.\nL. Y. Chew^\\dagger, J. W. Cheong, and A. Pradana, Enhancement of quantum processes from indefinite causal order through non-Markovianity, XLV Dynamics Days Europe 2025 (DDE2025), Thessaloniki, Greece, June 2025.\nL. Y. Chew^\\dagger, L. He, A. Pradana, and J. W. Cheong, Stochastic thermodynamics and correlation effects of finite-tape information ratchets, 28th International Conference on Statistical Physics (STATPHYS28), Tokyo, Japan, August 2023.\n\n\n\n^\\daggerPresenting author."
  },
  {
    "objectID": "cv.html#technical-experience",
    "href": "cv.html#technical-experience",
    "title": "Curriculum Vitae",
    "section": "Technical Experience",
    "text": "Technical Experience\n\nSoftware\nComputer languages  Python, Julia, R, C, C++, MATLAB, Haskell, Racket, LaTeX, Typst, HTML, CSS, bash, POSIX sh\nMiscellaneous software  Arduino, Google Sketchup, Autodesk Fusion360, Autodesk AutoCAD, Autodesk EAGLE, Origin Pro, National Instruments LabVIEW\n\n\nProjects\nStrain estimation for hazard forecastings before and after 2011 Japan Tohoku earthquake  ES7008 Geophysical Data Analysis, NTU  Analyzed seismic GPS displacement data in Python.  Estimated seismic strains with velocity fields using Delaunay triangulation.  Demonstrated correlations between earthquake event hotspots and strain hotspots, before and after Tohoku earthquake.\nVariations in statistical complexity of genome sequences across species  CE7412 Computational and Systems Biology, NTU  Analyzed genome sequences of human, chimpanzee, rhesus macaque, dog, and fruit fly, from GenBank assembly in R.  Applied the Baum-Welch algorithm and Akaike information criterion to compute the average statistical complexity of their genomes.  Suggested that increased biological complexity corresponds to decreased statistical complexity in genomes.\nDetecting adversarial attack of deep neural networks for image recognition from image complexity  PH3502 Chaotic Dynamical Systems, NTU  Trained image recognition deep neural networks with MNIST, Fashion-MNIST, and CIFAR10 datasets in Python.  Applied adversarial attacks such as Fast Gradient Sign Method (FGSM), DeepFool, One Pixel Attack, Jacobian-Based Saliency Map Attack (JSMA).  Showed that FGSM and DeepFool can be detected from its increased image complexity.\nMonte Carlo photon transport in multi-layered biological tissues  PH4505 Computational Physics, NTU  Simulated photon transports in biological tissues by means of random walk in Python.  Demonstrated the applications of computational methods on medical areas such as biomedical imaging and photon therapy.\nMonte Carlo simulation of periodic-driven Brownian particles  PAP723 Numerical Methods for Physicists, NTU  Simulated 2D toy model of attractive Brownian particles that obeys the Arrhenius equation for the formation and destruction of bonds.  Demonstrated that the system tends to configurations that result in increased entropy production when driven with a periodic driving force.\nDesigning, programming, 3D printing, and building a hovering quadcopter drone  Making and Tinkering Lite 1, NTU  Programmed a Arduino microcontroller.  Designed printed circuit board (PCB) in Autodesk EAGLE.  Designed and 3D printed drone in Autodesk Fusion360.  Simulated physical system in COMSOL Multiphysics."
  }
]